{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Better Deep Learning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMV/QpRHWYRgtVN314jsuzE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cvPZ_TGOMBTL"},"source":["CHAPTER 2:\n","\n","CONFIGURE CAPACITY WITH:\n","\n","1. NODES\n","\n","2. LAYERS\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8Cb3pbR76TsJ"},"source":["SECTION 2.3.2\n","\n","\n","Change Model Capacity with NODES\n","\n","--\n","--\n","--"]},{"cell_type":"code","metadata":{"id":"seKsnyq8L214"},"source":["# Study of Multilayer Perceptron learning curves given \n","# different number of \n","# nodes for multi-class classification\n","\n","from sklearn.datasets.samples_generator import make_blobs\n","from keras.layers import Dense\n","from keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.utils import to_categorical\n","from matplotlib import pyplot as plt\n","\n","\n","\n","\n","# Prepare multiclass classification dataset\n","\n","\n","def create_dataset():\n","\n","  # generate 2D classification dataset\n","\n","  X, y = make_blobs(n_samples=1000, centers=20, n_features=100, \n","                    \n","                    cluster_std=2, random_state=2)\n","\n","  # one-hot encode output variable\n","\n","  y = to_categorical(y)\n","\n","  # split into train and test\n","\n","  n_train = 500\n","\n","  train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","  train_y, test_y = y[:n_train], y[n_train:]\n","\n","  return train_X, train_y, test_X, test_y\n","\n","\n","\n","\n","# fit model with given number of nodes , returns test set accuracy\n","\n","\n","def evaluate_model(n_nodes, train_X, train_y, test_X, test_y):\n","\n","  # configure the model, based on data\n","\n","  n_input, n_classes = train_X.shape[1], test_y.shape[1]\n","\n","  # define model\n","\n","  model = Sequential()\n","\n","  model.add(Dense(n_nodes, input_dim=n_input, activation='relu', \n","                  \n","                  kernel_initializer='he_uniform'))\n","\n","  model.add(Dense(n_classes, activation='softmax'))\n","\n","  # compile model\n","\n","  opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","  model.compile(loss='categorical_crossentropy', optimizer=opt, \n","                \n","                metrics=['accuracy'])\n","  \n","  # fit model on train set\n","\n","  history = model.fit(train_X, train_y, epochs=100, verbose=0)\n","\n","  # evaluate model on test set\n","\n","  _, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","  return history, test_acc\n","\n","\n","\n","\n","# prepare dataset\n","\n","\n","train_X, train_y, test_X, test_y = create_dataset()\n","\n","\n","\n","\n","# evaluate model and plot learning curve with given number of nodes\n","\n","\n","num_nodes = [1, 2, 3, 4, 5, 6, 7]\n","\n","for n_nodes in num_nodes:\n","\n","  # evaluate model with a given number of nodes\n","\n","  history, result = evaluate_model(n_nodes, \n","                                   \n","                                   train_X, train_y, test_X, test_y)\n","\n","  # summarise final test set accuracy\n","\n","  print('nodes=%d: %.3f' % (n_nodes, result))\n","\n","  # plot learning curve\n","\n","  plt.plot(history.history['loss'], label=str(n_nodes))\n","\n","\n","\n","\n","  # show the plot\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dyJwmyP6Txrb"},"source":["--\n","--\n","--\n","\n","SECTION 2.3.3  \n","\n","\n","Change Model Capacity with LAYERS\n","\n","--\n","--\n","--"]},{"cell_type":"code","metadata":{"id":"YyQOEoWxT6fH"},"source":["# study of MLP learning curves given different number of layers\n","# for multiclass classification\n","\n","\n","from sklearn.datasets.samples_generator import make_blobs\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.utils import to_categorical\n","from matplotlib import pyplot as plt\n","\n","\n","# prepare multiclass classification dataset\n","\n","\n","def create_dataset():\n","\n","    # generate 2D classification dataset\n","\n","    X,y = make_blobs(n_samples=1000, centers=20,\n","                     \n","                     n_features=100, cluster_std=2,\n","\n","                     random_state=2)\n","    \n","    # one hot encode output variable\n","\n","    y = to_categorical(y)\n","\n","    # split into train and test\n","\n","    n_train = 500\n","\n","    train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","    train_y, test_y = y[:n_train], y[n_train:]\n","\n","    return train_X, train_y, test_X, test_y\n","\n","\n","\n","# fit model with given number of layers, return test accuracy\n","\n","\n","def evaluate_model(n_layers, train_X, train_y, test_X, test_y):\n","\n","    # configure the model based on the data\n","\n","    n_input, n_classes = train_X.shape[1], test_y.shape[1]\n","\n","    # define model\n","\n","    model = Sequential()\n","\n","    model.add(Dense(10, activation='relu', \n","                    \n","                    kernel_initializer='he_uniform'))\n","    \n","    model.add(Dense(n_classes, activation='softmax'))\n","\n","    # compile model\n","\n","    opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  \n","                  optimizer=opt, metrics=['accuracy'])\n","    \n","    # fit model\n","\n","    history = model.fit(train_X, train_y, epochs=100, \n","                        \n","                        verbose = 0)\n","    \n","    # evaluate model on test set\n","\n","    _, test_acc = model.evaluate(test_X, test_y, \n","                                 \n","                                 verbose=0)\n","    \n","    return history, test_acc\n","\n","\n","\n","# get dataset\n","\n","train_X, train_y, test_X, test_y = create_dataset()\n","\n","\n","\n","# evaluate model and plot learning curve of model\n","# with given number pf layers\n","\n","\n","\n","all_history = list()\n","\n","num_layers = [1, 2, 3, 4, 5]\n","\n","\n","\n","for n_layers in num_layers:\n","\n","    # evaluate model with a given number of layers\n","\n","    history, result = evaluate_model(n_layers, train_X,\n","                                     \n","                                     train_y, test_X, \n","                                     \n","                                     test_y)\n","    \n","    print('layers=%d: %.3f' % (n_layers, result))\n","\n","    # plot learning curve\n","\n","    plt.plot(history.history['loss'], label=str(n_layers))\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xAxUWNBxW731"},"source":["CHAPTER 3:\n","\n","CONFIGURE GRADIENT PRECISION WITH BATCH SIZE\n","\n","--\n","--\n","--\n","\n","POINT 1:\n","\n","The NUMBER OF EXAMPLES in the TRAINING DATASET used in the estimation of the ERROR GRADIENT is called the BATCH SIZE\n","\n","\n","POINT 2:\n","\n","THREE different flavours of gradient descent are Batch, Stochastic and Minibatc Gradient Descent \n","\n","--\n","--\n","--\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DnuXUXxf8p5H"},"source":["3.3.2 Multi-Layer Perceptron with Batch Gradient Descent\n","\n","--\n","--\n","\n","This would be used to address a Multiclass Classification Problem.\n","\n","--\n","--\n","\n"]},{"cell_type":"code","metadata":{"id":"mE4n-0HuW_6P"},"source":["# Multi-Layer Perceptron for the blobs \n","#problem with batch gradient descent\n","\n","from sklearn.datasets.samples_generator import make_blobs\n","from keras.layers import Dense\n","from keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.utils import to_categorical\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2D classification dataset\n","\n","X, y = make_blobs(n_samples=1000, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","\n","                  random_state=2)\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(50, input_dim=2, \n","                \n","                activation='relu',\n","                \n","                kernel_initializer='he_uniform')) \n","\n","                # Note that '50' is the number of nodes \n","                # in the layer\n","\n","model.add(Dense(3, \n","                \n","                activation='softmax'))\n","\n","\n","# Compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='categorical_crossentropy',\n","              \n","              optimizer=opt, metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=200, verbose=0,\n","                    \n","                    batch_size=len(train_X))\n","\n","\n","\n","# evaluate model\n","\n","_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss learning curve\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","# Plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5c9GsMr7Cn8y"},"source":["--\n","\n","--\n","\n","3.3.3 Multi-Layer Perceptron fit \n","\n","with Stochastic Gradient Descent\n","\n","--\n","\n","--\n","\n"]},{"cell_type":"code","metadata":{"id":"cEW8BxGrC9rh"},"source":["# Multi-Layer Perceptron for the blobs \n","#problem with STOCHASTIC gradient descent\n","\n","from sklearn.datasets.samples_generator import make_blobs\n","from keras.layers import Dense\n","from keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.utils import to_categorical\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2D classification dataset\n","\n","X, y = make_blobs(n_samples=1000, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","\n","                  random_state=2)\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(50, input_dim=2, \n","                \n","                activation='relu',\n","                \n","                kernel_initializer='he_uniform')) \n","\n","                # Note that '50' is the number of nodes \n","                # in the layer\n","\n","model.add(Dense(3, \n","                \n","                activation='softmax'))\n","\n","\n","# Compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='categorical_crossentropy',\n","              \n","              optimizer=opt, metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=200, verbose=0,\n","                    \n","                    batch_size=1)\n","\n","\n","\n","# evaluate model\n","\n","_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss learning curve\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","# Plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v1g4HISjE2vg"},"source":["--\n","\n","--\n","\n","The poor performance and the erratic changes to the model ABOVE (in 3.3.3, as seen in the resulting graphs) suggest that the LEARNING RATE used to update weights after each training example may be TOO LARGE, and that a SMALLER learning rate may make the LEARNING PROCESS more stable.\n","\n","--\n","\n","--\n","\n","We can TEST this by re-running the model fit with stochastic gradient descent and a SMALLER LEARNING RATE. \n","\n","--\n","\n","--\n","\n","For example, we can reduce the learning rate from 0.01 to 0.001.\n","\n","--\n","\n","--\n","\n","Let us see\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"b0HAkHNWGPkk"},"source":["# Multi-Layer Perceptron for the blobs \n","#problem with STOCHASTIC gradient descent (SMALLER LEARNING RATE)\n","\n","from sklearn.datasets.samples_generator import make_blobs\n","from keras.layers import Dense\n","from keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.utils import to_categorical\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2D classification dataset\n","\n","X, y = make_blobs(n_samples=1000, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","\n","                  random_state=2)\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(50, input_dim=2, \n","                \n","                activation='relu',\n","                \n","                kernel_initializer='he_uniform')) \n","\n","                # Note that '50' is the number of nodes \n","                # in the layer\n","\n","model.add(Dense(3, \n","                \n","                activation='softmax'))\n","\n","\n","# Compile model\n","\n","opt = SGD(learning_rate=0.001, momentum=0.9)\n","\n","model.compile(loss='categorical_crossentropy',\n","              \n","              optimizer=opt, metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=200, verbose=0,\n","                    \n","                    batch_size=1)\n","\n","\n","\n","# evaluate model\n","\n","_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss learning curve\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","# Plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y_SDR2wHGxeG"},"source":["--\n","\n","--\n","\n","3.3.4 Multi-Layer Perceptron fit with minibatch gradient descent\n","\n","--\n","\n","--\n","\n","An ALTERNATIVE to using stochastic gradient descent and tuning the learning rate is to HOLD THE LEARNING RATE CONSTANT and CHANGE THE BATCH SIZE.\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"1SoLtLP6Ioqb"},"source":["# Multi-Layer Perceptron for the blobs \n","#problem with MINIBATCH gradient descent\n","\n","from sklearn.datasets.samples_generator import make_blobs\n","from keras.layers import Dense\n","from keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.utils import to_categorical\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2D classification dataset\n","\n","X, y = make_blobs(n_samples=1000, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","\n","                  random_state=2)\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(50, input_dim=2, \n","                \n","                activation='relu',\n","                \n","                kernel_initializer='he_uniform')) \n","\n","                # Note that '50' is the number of nodes \n","                # in the layer\n","\n","model.add(Dense(3, \n","                \n","                activation='softmax'))\n","\n","\n","# Compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='categorical_crossentropy',\n","              \n","              optimizer=opt, metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=200, verbose=0,\n","                    \n","                    batch_size=32)\n","\n","\n","\n","# evaluate model\n","\n","_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss learning curve\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","# Plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UHjV6xRGJGhY"},"source":["--\n","\n","--\n","\n","--\n","\n","3.3.5 Effect of batch size on Model behaviour\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"dQs8eDWgLZoi"},"source":["# Multi-Layer Perceptron for the BLOBS problem \n","# with minibatch gradient descent with VARIED BATCH SIZE\n","\n","from sklearn.datasets.samples_generator import make_blobs\n","from keras.layers import Dense\n","from keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.utils import to_categorical\n","from matplotlib import pyplot as plt\n","\n","\n","# Prepare train and test dataset\n","\n","def prepare_data():\n","\n","    # generate 2D classification dataset\n","\n","    X, y = make_blobs(n_samples=1000, centers=3, \n","                      \n","                      n_features=2, cluster_std=2,\n","                      \n","                      random_state=2)\n","    \n","    # one hot encode output variable\n","\n","    y = to_categorical(y)\n","\n","    # split into train and test set\n","\n","    n_train = 500\n","\n","    train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","    train_y, test_y = y[:n_train], y[n_train:]\n","\n","    return train_X, train_y, test_X, test_y\n","\n","\n","\n","\n","# fit a model and plot learning curve\n","\n","def fit_model(train_X, train_y, test_X, test_y, n_batch):\n","\n","    # define model\n","\n","    model = Sequential()\n","\n","    model.add(Dense(50, input_dim=2, activation='relu',\n","                    \n","                    kernel_initializer='he_uniform'))\n","    \n","    model.add(Dense(3, activation='softmax'))\n","\n","    # compile model\n","\n","    opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  \n","                  optimizer=opt, metrics=['accuracy'])\n","    \n","    # fit model\n","\n","    history = model.fit(train_X, train_y, \n","                        \n","                        validation_data=(test_X, test_y),\n","                        \n","                        epochs=200, verbose=0,\n","                        \n","                        batch_size=n_batch)\n","    \n","    # plot learning curves\n","\n","    plt.plot(history.history['accuracy'], label='train')\n","\n","    plt.plot(history.history['val_accuracy'], label='test')\n","\n","    plt.title('batch=' + str(n_batch), pad=-40)\n","\n","\n","\n","\n","# prepare dataset\n","\n","train_X, train_y, test_X, test_y = prepare_data()\n","\n","# create learning curves for different batch sizes.\n","\n","batch_sizes = [4, 8, 16, 32, 64, 128, 256, 450]\n","\n","\n","\n","\n","for i in range(len(batch_sizes)):\n","\n","    # determine the plot number\n","\n","    plot_number = 420 + (i + 1)\n","\n","    plt.subplot(plot_number)\n","\n","    # fit model and plot learning curves for a batch size\n","\n","    fit_model(train_X, train_y, test_X, test_y, batch_sizes[i])\n","\n","\n","\n","\n","# show learning curves\n","\n","plt.show\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OiJNM2j8PQtv"},"source":["--\n","\n","--\n","\n","4.2.1 \n","\n","Mean Squared Error Loss\n","\n","(MSE)\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"G04jprxJPRgM"},"source":["# Multi-Layer Perceptron with Mean Squared Error\n","# loss function\n","\n","from sklearn.datasets import make_regression\n","from sklearn.preprocessing import StandardScaler\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","\n","\n","# generate regression dataset\n","\n","X, y = make_regression(n_samples=1000, n_features=20,\n","                       \n","                       noise=0.1,\n","                       \n","                       random_state=1)\n","\n","# Standardize dataset\n","\n","X = StandardScaler().fit_transform(X)\n","\n","y = StandardScaler().fit_transform(y.reshape(len(y), 1))[:, 0]\n","\n","\n","# split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(25, input_dim=20, activation='relu',\n","                \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(1, activation='linear'))\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='mean_squared_error', \n","              \n","              optimizer=opt)\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=100, verbose=0)\n","\n","\n","# evaluate the model\n","\n","train_mse = model.evaluate(train_X, train_y, \n","                           \n","                           verbose=0)\n","\n","test_mse = model.evaluate(test_X, test_y, \n","                             \n","                             verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n","\n","\n","\n","\n","# plot loss during training\n","\n","plt.title('Mean Squared Error Loss')\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LwmVDT3VZB_m"},"source":["--\n","\n","--\n","\n","4.2.2\n","\n","Mean Squared Logarithmic Error Loss\n","\n","\n","--\n","\n","\n","--"]},{"cell_type":"code","metadata":{"id":"2tPKC-3RZCj3"},"source":["# Multi-Layer Perceptron with Mean Squared Error\n","# loss function\n","\n","from sklearn.datasets import make_regression\n","from sklearn.preprocessing import StandardScaler\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","\n","\n","# generate regression dataset\n","\n","X, y = make_regression(n_samples=1000, n_features=20,\n","                       \n","                       noise=0.1,\n","                       \n","                       random_state=1)\n","\n","# Standardize dataset\n","\n","X = StandardScaler().fit_transform(X)\n","\n","y = StandardScaler().fit_transform(y.reshape(len(y), 1))[:, 0]\n","\n","\n","# split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(25, input_dim=20, activation='relu',\n","                \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(1, activation='linear'))\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='mean_squared_logarithmic_error', \n","              \n","              optimizer=opt, metrics=['mse'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=100, verbose=0)\n","\n","\n","# evaluate the model\n","\n","_, train_mse = model.evaluate(train_X, train_y, \n","                           \n","                           verbose=0)\n","\n","_,test_mse = model.evaluate(test_X, test_y, \n","                             \n","                             verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n","\n","\n","\n","\n","# plot loss during training\n","\n","plt.subplot(211)\n","\n","plt.title('Mean Squared Logarithmic Error Loss', pad=-20)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n","\n","# plot mse during training\n","\n","plt.subplot(212)\n","\n","plt.title('Mean Squared Error', pad=-20)\n","\n","plt.plot(history.history['mse'], label='train')\n","\n","plt.plot(history.history['val_mse'], label='test')\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TdQbOmaklpr4"},"source":["--\n","\n","\n","--\n","\n","\n","--\n","\n","\n","4.2.3 \n","\n","Mean Absolute Error Loss\n","\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"t2_eBumwlqH6"},"source":["# Multi-Layer Perceptron with Mean Squared Error\n","# loss function\n","\n","from sklearn.datasets import make_regression\n","from sklearn.preprocessing import StandardScaler\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","\n","\n","# generate regression dataset\n","\n","X, y = make_regression(n_samples=1000, n_features=20,\n","                       \n","                       noise=0.1,\n","                       \n","                       random_state=1)\n","\n","# Standardize dataset\n","\n","X = StandardScaler().fit_transform(X)\n","\n","y = StandardScaler().fit_transform(y.reshape(len(y), 1))[:, 0]\n","\n","\n","# split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(25, input_dim=20, activation='relu',\n","                \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(1, activation='linear'))\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='mean_absolute_error', \n","              \n","              optimizer=opt, metrics=['mse'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=100, verbose=0)\n","\n","\n","# evaluate the model\n","\n","_, train_mse = model.evaluate(train_X, train_y, \n","                           \n","                           verbose=0)\n","\n","_,test_mse = model.evaluate(test_X, test_y, \n","                             \n","                             verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n","\n","\n","\n","\n","# plot loss during training\n","\n","plt.subplot(211)\n","\n","plt.title('Mean Absolute Error Loss', pad=-20)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n","\n","# plot mse during training\n","\n","plt.subplot(212)\n","\n","plt.title('Mean Squared Error', pad=-20)\n","\n","plt.plot(history.history['mse'], label='train')\n","\n","plt.plot(history.history['val_mse'], label='test')\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G6c2zezEmZMp"},"source":["--\n","\n","--\n","\n","--\n","\n","4.3.\n","\n","Binary Classification Loss Function Case Study\n","\n","--\n","\n","--\n","\n","--\n","\n","4.3.1 \n","\n","Binary Cross-Entropy Loss\n","\n","--\n","\n","--\n","--"]},{"cell_type":"code","metadata":{"id":"1EGrsL9EmZdo"},"source":["# Multi-Layer Perceptron for the circles \n","# problem with cross-entropy loss\n","\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","\n","\n","# generate 2D classification dataset\n","\n","X,y = make_circles(n_samples=1000, noise=0.1,\n","                   \n","                   random_state=1)\n","\n","\n","# split into train and test\n","\n","n_train=500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(50, input_dim=2, activation='relu',\n","                \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='binary_crossentropy', optimizer=opt,\n","              \n","              metrics=['accuracy'])\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=200, verbose=0)\n","\n","\n","# evaluate the model\n","\n","\n","_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss during training\n","\n","plt.subplot(211)\n","\n","plt.title('Binary Cross-Entropy Loss', pad=-20)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","\n","\n","# plot accuracy during training\n","\n","\n","plt.subplot(212)\n","\n","plt.title('Classification Accuracy', pad=-20)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","plt.show()\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xPWk5S380MFL"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","4.3.2 Hinge Loss\n","\n","--\n","\n","--\n","\n","--\n","\n","\n","--\n"]},{"cell_type":"code","metadata":{"id":"EHkyRt_H0MXf"},"source":["# Multi-Layer Perceptron for the circles \n","# problem with hinge loss\n","\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","from numpy import where\n","\n","\n","# generate 2D classification dataset\n","\n","X,y = make_circles(n_samples=1000, noise=0.1,\n","                   \n","                   random_state=1)\n","\n","\n","# Change y from {0,1} to {-1, 1}\n","\n","y[where(y == 0)] = -1\n","\n","\n","\n","# split into train and test\n","\n","n_train=500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(50, input_dim=2, activation='relu',\n","                \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(1, activation='tanh'))\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='hinge', optimizer=opt,\n","              \n","              metrics=['accuracy'])\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=200, verbose=0)\n","\n","\n","# evaluate the model\n","\n","\n","_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss during training\n","\n","plt.subplot(211)\n","\n","plt.title('Hinge Loss', pad=-20)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","\n","\n","# plot accuracy during training\n","\n","\n","plt.subplot(212)\n","\n","plt.title('Classification Accuracy', pad=-20)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","plt.show()\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-z-V3DhF1WA-"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","4.3.3\n","\n","Squared Hinge Loss\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"-6Gjnd3a1WSt"},"source":["# Multi-Layer Perceptron for the circles \n","# problem with Squared hinge loss\n","\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","from numpy import where\n","\n","\n","# generate 2D classification dataset\n","\n","X,y = make_circles(n_samples=1000, noise=0.1,\n","                   \n","                   random_state=1)\n","\n","\n","# Change y from {0,1} to {-1, 1}\n","\n","y[where(y == 0)] = -1\n","\n","\n","\n","# split into train and test\n","\n","n_train=500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(50, input_dim=2, activation='relu',\n","                \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(1, activation='tanh'))\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='squared_hinge', optimizer=opt,\n","              \n","              metrics=['accuracy'])\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=200, verbose=0)\n","\n","\n","# evaluate the model\n","\n","\n","_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss during training\n","\n","plt.subplot(211)\n","\n","plt.title('Squared Hinge Loss', pad=-20)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","\n","\n","# plot accuracy during training\n","\n","\n","plt.subplot(212)\n","\n","plt.title('Classification Accuracy', pad=-20)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZAUq7ed42fH0"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","\n","4.4.1\n","\n","Multi-Class Cross-Entropy loss\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"8cdP82zZ2foJ"},"source":["# Multi-Layer Perceptron for the blobs \n","# multi-class classification problem with\n","# cross-entropy loss\n","\n","from sklearn.datasets.samples_generator import make_blobs\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.utils import to_categorical\n","from matplotlib import pyplot as plt\n","\n","\n","# generate 2D classification dataset\n","\n","X, y = make_blobs(n_samples=1000, centers=3,\n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","# split into train and test set\n","\n","n_train=500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(50, input_dim=2, activation='relu',\n","                \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(3, activation='softmax'))\n","\n","\n","\n","# compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='categorical_crossentropy', \n","              \n","              optimizer=opt, metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=100, verbose=0)\n","\n","\n","\n","# evaluate the model\n","# evaluate the model\n","\n","\n","_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss during training\n","\n","plt.subplot(211)\n","\n","plt.title('Categorical Cross-Entropy Loss', pad=-20)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","\n","\n","# plot accuracy during training\n","\n","\n","plt.subplot(212)\n","\n","plt.title('Classification Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-jHLot5hKMSc"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","\n","4.4.2 \n","\n","Sparse Multiclass Cross-Entropy Loss\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"mjgdmwPgKMlq"},"source":["# Multi-Layer Perceptron for the blobs \n","# multi-class classification problem with\n","# sparse cross-entropy loss\n","\n","from sklearn.datasets.samples_generator import make_blobs\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","\n","\n","# generate 2D classification dataset\n","\n","X, y = make_blobs(n_samples=1000, centers=3,\n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","# split into train and test set\n","\n","n_train=500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(50, input_dim=2, activation='relu',\n","                \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(3, activation='softmax'))\n","\n","\n","\n","# compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='sparse_categorical_crossentropy', \n","              \n","              optimizer=opt, metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=100, verbose=0)\n","\n","\n","\n","# evaluate the model\n","# evaluate the model\n","\n","\n","_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss during training\n","\n","plt.subplot(211)\n","\n","plt.title('Sparse Categorical Cross-Entropy Loss', pad=-20)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","\n","\n","# plot accuracy during training\n","\n","\n","plt.subplot(212)\n","\n","plt.title('Classification Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FNa5Ow5NLgCM"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","\n","4.4.3\n","\n","Kullback Leibler Divergence Loss\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"547MU4tzLgSp"},"source":["# Multi-Layer Perceptron for the blobs \n","# multi-class classification problem with\n","# Kullback Leibler devergence loss\n","\n","from sklearn.datasets.samples_generator import make_blobs\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.utils import to_categorical\n","from matplotlib import pyplot as plt\n","\n","\n","# generate 2D classification dataset\n","\n","X, y = make_blobs(n_samples=1000, centers=3,\n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","# split into train and test set\n","\n","n_train=500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(50, input_dim=2, activation='relu',\n","                \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(3, activation='softmax'))\n","\n","\n","\n","# compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='kullback_leibler_divergence', \n","              \n","              optimizer=opt, metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=100, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","\n","_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss during training\n","\n","plt.subplot(211)\n","\n","plt.title('Kullback Leibler Divergence Loss', pad=-20)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","\n","\n","# plot accuracy during training\n","\n","\n","plt.subplot(212)\n","\n","plt.title('Classification Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l_Ls3NG-NEVk"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","5.3.2\n","\n","Effect of Leaning Rate and Momentum\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","A. \n","\n","Learning Rate Dynamics\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"35_agGDwNE9E"},"source":["# Study on the effects of Learning Rate on Accuracy \n","# for the blobs problem\n","\n","\n","from sklearn.datasets.samples_generator import make_blobs\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.utils import to_categorical\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# COPIED from 2.3.3\n","\n","\n","# prepare train and test dataset\n","\n","\n","def prepare_data():\n","\n","    # generate 2D classification dataset\n","\n","    X,y = make_blobs(n_samples=1000, centers=3,\n","                     \n","                     n_features=2, cluster_std=2,\n","\n","                     random_state=2)\n","    \n","    # one hot encode output variable\n","\n","    y = to_categorical(y)\n","\n","    # split into train and test\n","\n","    n_train = 500\n","\n","    train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","    train_y, test_y = y[:n_train], y[n_train:]\n","\n","    return train_X, train_y, test_X, test_y\n","\n","\n","\n","\n","# fit a model and plot learning curve\n","\n","\n","def fit_model(train_X, train_y, test_X, test_y, lrate):\n","\n","    # define model\n","\n","    model = Sequential()\n","\n","    model.add(Dense(50, input_dim=2, activation='relu', \n","                    \n","                    kernel_initializer='he_uniform'))\n","    \n","    model.add(Dense(3, activation='softmax'))\n","\n","    # compile model\n","\n","    opt = SGD(learning_rate=lrate)\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  \n","                  optimizer=opt, metrics=['accuracy'])\n","    \n","    # fit model\n","\n","    history = model.fit(train_X, train_y, \n","                        \n","                        validation_data=(test_X, test_y),\n","                        \n","                        epochs=200, \n","                        \n","                        verbose=0)\n","    \n","    # plot learning curves\n","\n","    plt.plot(history.history['accuracy'], label='train')\n","\n","    plt.plot(history.history['val_accuracy'], label='test')\n","\n","    plt.title('lrate=' + str(lrate), pad=-50)\n","\n","\n","\n","# prepare dataset\n","\n","train_X, train_y, test_X, test_y = prepare_data()\n","\n","\n","\n","# create learning curves for different learning rates\n","\n","learning_rates = [1E-0, 1E-1, 1E-2, 1E-3, \n","                  \n","                  1E-4, 1E-5, 1E-6, 1E-7]\n","\n","\n","for i in range(len(learning_rates)):\n","\n","    # Determine the plot number\n","\n","    plot_no = 420 + (i+1)\n","\n","    plt.subplot(plot_no)\n","\n","    # fit model and plot learning curves for a learning rate\n","\n","    fit_model(train_X, train_y, \n","              \n","              test_X, test_y, learning_rates[i])\n","    \n","\n","\n","\n","# show leaning curves\n","\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uprkT0iCQqi7"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","5.3.2\n","\n","B.\n","\n","Momentum Dynamics\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"XVcqSYQlQq_x"},"source":["# Study on the effects of Momentum on Accuracy \n","# for the blobs problem\n","\n","\n","from sklearn.datasets.samples_generator import make_blobs\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.utils import to_categorical\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# COPIED from 5.3.2 A: Learning Rate Dynamics\n","\n","\n","# prepare train and test dataset\n","\n","\n","def prepare_data():\n","\n","    # generate 2D classification dataset\n","\n","    X,y = make_blobs(n_samples=1000, centers=3,\n","                     \n","                     n_features=2, cluster_std=2,\n","\n","                     random_state=2)\n","    \n","    # one hot encode output variable\n","\n","    y = to_categorical(y)\n","\n","    # split into train and test\n","\n","    n_train = 500\n","\n","    train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","    train_y, test_y = y[:n_train], y[n_train:]\n","\n","    return train_X, train_y, test_X, test_y\n","\n","\n","\n","\n","# fit a model and plot learning curve\n","\n","\n","def fit_model(train_X, train_y, test_X, test_y, momentum):\n","\n","    # define model\n","\n","    model = Sequential()\n","\n","    model.add(Dense(50, input_dim=2, activation='relu', \n","                    \n","                    kernel_initializer='he_uniform'))\n","    \n","    model.add(Dense(3, activation='softmax'))\n","\n","    # compile model\n","\n","    opt = SGD(learning_rate=0.01, momentum=momentum)\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  \n","                  optimizer=opt, metrics=['accuracy'])\n","    \n","    # fit model\n","\n","    history = model.fit(train_X, train_y, \n","                        \n","                        validation_data=(test_X, test_y),\n","                        \n","                        epochs=200, \n","                        \n","                        verbose=0)\n","    \n","    # plot learning curves\n","\n","    plt.plot(history.history['accuracy'], label='train')\n","\n","    plt.plot(history.history['val_accuracy'], label='test')\n","\n","    plt.title('lrate=' + str(momentums), pad=-80)\n","\n","\n","\n","# prepare dataset\n","\n","train_X, train_y, test_X, test_y = prepare_data()\n","\n","\n","\n","# create learning curves for different momentums\n","\n","momentums = [0.0, 0.5, 0.9, 0.99]\n","\n","\n","for i in range(len(momentums)):\n","\n","    # Determine the plot number\n","\n","    plot_no = 220 + (i+1)\n","\n","    plt.subplot(plot_no)\n","\n","    # fit model and plot learning curves for a momentum\n","\n","    fit_model(train_X, train_y, \n","              \n","              test_X, test_y, momentums[i])\n","    \n","\n","\n","\n","# show leaning curves\n","\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KHN2dpMdT6p2"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","5.3.3\n","\n","Effect of Learning Rate Schedules\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","A. \n","\n","Learning Rate Decay\n","\n","(An Experiment)\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"ja0QT1utT65B"},"source":["# demonstrate the effect of \n","# decay on the learning rate\n","\n","from matplotlib import pyplot as plt\n","\n","\n","\n","\n","# learning rate decay\n","\n","def decay_lrate(initial_lrate, decay, iteration):\n","\n","    result = initial_lrate * (1.0 / (1.0 + (decay * iteration)))\n","\n","    return  result\n","\n","\n","decays = [1E-1, 1E-2, 1E-3, 1E-4]\n","\n","lrate = 0.01\n","\n","n_updates = 200\n","\n","for decay in decays:\n","\n","    # calculate the learning rates for updates\n","\n","    lrates = [decay_lrate(lrate, decay, i) \n","    \n","                for i in range(n_updates)]\n","\n","    # plot result\n","\n","    plt.plot(lrates, label=str(decay))\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BbF6EMfSW_ca"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","5.3.3\n","\n","B.\n","\n","page 107\n","\n","Evaluate the same 4 decay values of   [1E-1, 1E-2, 1E-3, 1E-4]   and their effects on model accuracy\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"KIAXlsb6W_tQ"},"source":["# study on the effects of decay rate on accuracy \n","# for the blobs problem\n","\n","from sklearn.datasets.samples_generator import make_blobs\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.utils import to_categorical\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# COPIED from 5.3.2 B: Momentum Dynamics\n","\n","\n","# prepare train and test dataset\n","\n","\n","def prepare_data():\n","\n","    # generate 2D classification dataset\n","\n","    X,y = make_blobs(n_samples=1000, centers=3,\n","                     \n","                     n_features=2, cluster_std=2,\n","\n","                     random_state=2)\n","    \n","    # one hot encode output variable\n","\n","    y = to_categorical(y)\n","\n","    # split into train and test\n","\n","    n_train = 500\n","\n","    train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","    train_y, test_y = y[:n_train], y[n_train:]\n","\n","    return train_X, train_y, test_X, test_y\n","\n","\n","\n","\n","# fit a model and plot learning curve\n","\n","\n","def fit_model(train_X, train_y, test_X, test_y, decay):\n","\n","    # define model\n","\n","    model = Sequential()\n","\n","    model.add(Dense(50, input_dim=2, activation='relu', \n","                    \n","                    kernel_initializer='he_uniform'))\n","    \n","    model.add(Dense(3, activation='softmax'))\n","\n","    # compile model\n","\n","    opt = SGD(learning_rate=0.01, decay=decay)\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  \n","                  optimizer=opt, metrics=['accuracy'])\n","    \n","    # fit model\n","\n","    history = model.fit(train_X, train_y, \n","                        \n","                        validation_data=(test_X, test_y),\n","                        \n","                        epochs=200, \n","                        \n","                        verbose=0)\n","    \n","    # plot learning curves\n","\n","    plt.plot(history.history['accuracy'], label='train')\n","\n","    plt.plot(history.history['val_accuracy'], label='test')\n","\n","    plt.title('decay=' + str(decay), pad=-80)\n","\n","\n","\n","# prepare dataset\n","\n","train_X, train_y, test_X, test_y = prepare_data()\n","\n","\n","\n","# create learning curves for different decay rates\n","\n","decay_rates = [1E-1, 1E-2, 1E-3, 1E-4]\n","\n","\n","for i in range(len(decay_rates)):\n","\n","    # Determine the plot number\n","\n","    plot_no = 220 + (i+1)\n","\n","    plt.subplot(plot_no)\n","\n","    # fit model and plot learning curves for a decay rate\n","\n","    fit_model(train_X, train_y, \n","              \n","              test_X, test_y, decay_rates[i])\n","    \n","\n","\n","\n","# show leaning curves\n","\n","\n","plt.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TFaOd1mZaNcw"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","\n","5.3.3 \n","\n","page 108, code on page 110\n","\n","C. \n","\n","Drop Learning Rate on Plateau\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"moMn2OwqaNsw"},"source":["# study on the effects of the \n","# patience of the learning rate drop schedule\n","# on the blobs problem.\n","\n","\n","from sklearn.datasets.samples_generator import make_blobs\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import Callback\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras import backend\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# monitor the learning rate\n","\n","class learning_rate_monitor(Callback):\n","\n","    # start of training\n","\n","    def on_train_begin(self, logs={}):\n","        self.lrates=list()\n","    \n","    # end of each training epoch\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","\n","        # get and store the learning rate\n","\n","        optimizer = self.model.optimizer\n","\n","        lrate = float(backend.get_value(optimizer.learning_rate))\n","\n","        self.lrates.append(lrate)\n","\n","\n","\n","# prepare train and test dataset\n","\n","\n","def prepare_data():\n","\n","    # generate 2D classification dataset\n","\n","    X,y = make_blobs(n_samples=1000, centers=3,\n","                     \n","                     n_features=2, cluster_std=2,\n","\n","                     random_state=2)\n","    \n","    # one hot encode output variable\n","\n","    y = to_categorical(y)\n","\n","    # split into train and test\n","\n","    n_train = 500\n","\n","    train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","    train_y, test_y = y[:n_train], y[n_train:]\n","\n","    return train_X, train_y, test_X, test_y\n","\n","\n","\n","\n","# fit a model and plot learning curve\n","\n","\n","def fit_model(train_X, train_y, test_X, test_y, patience):\n","\n","    # define model\n","\n","    model = Sequential()\n","\n","    model.add(Dense(50, input_dim=2, activation='relu', \n","                    \n","                    kernel_initializer='he_uniform'))\n","    \n","    model.add(Dense(3, activation='softmax'))\n","\n","    # compile model\n","\n","    opt = SGD(learning_rate=0.01)\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  \n","                  optimizer=opt, metrics=['accuracy'])\n","    \n","    # fit model\n","\n","    rlrp = ReduceLROnPlateau(monitor='val_loss',\n","                             \n","                             factor=0.1,\n","                             \n","                             patience=patience,\n","                             \n","                             min_delta=1E-7)\n","    \n","    lrm = learning_rate_monitor()\n","\n","\n","    history = model.fit(train_X, train_y, \n","                        \n","                        validation_data=(test_X, test_y),\n","                        \n","                        epochs=200, verbose=0,\n","                        \n","                        callbacks=[rlrp, lrm])\n","    \n","    return lrm.lrates, history.history['loss'], history.history['accuracy']\n","    \n","\n","\n","# create line plot for a series\n","\n","def line_plots(patiences, series):\n","\n","    for i in range (len(patiences)):\n","\n","        plt.subplot(220 + (i + 1))\n","\n","        plt.plot(series[i])\n","\n","        plt.title('patience=' + str(patiences[i]), pad=-80)\n","\n","    plt.show()\n","\n","\n","\n","# prepare dataset\n","\n","train_X, train_y, test_X, test_y = prepare_data()\n","\n","\n","# create learning curves for different patiences\n","\n","patiences = [2, 5, 10, 15]\n","\n","lr_list, loss_list, accuracy_list = list(), list(), list()\n","\n","for i in range(len(patiences)):\n","\n","    # fit model and plot learning curves for a patience\n","    \n","    lr, loss, accuracy = fit_model(train_X, train_y,\n","                                              \n","                                    test_X, test_y, \n","                                              \n","                                    patiences[i])\n","\n","    lr_list.append(lr)\n","\n","    loss_list.append(loss)\n","\n","    accuracy_list.append(accuracy)\n","\n","\n","\n","\n","\n","\n","# plot learning rates\n","\n","line_plots(patiences, lr_list)\n","\n","# plot loss\n","\n","line_plots(patiences, loss_list)\n","\n","# plot accuracy\n","\n","line_plots(patiences, accuracy_list)\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1rX-yRct6I-N"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","5.3.4\n","\n","page 115, code on page 116\n","\n","Effect of Adaptive Learning Rates\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"j1GUZ5Xf6JQl"},"source":["# study of Stochastic Gradient Descent (SGD) \n","# with Adaptive Learning Rates for the blobs problem\n","\n","from sklearn.datasets.samples_generator import make_blobs\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","from matplotlib import pyplot as plt\n","\n","\n","\n","\n","# prepare train and test dataset\n","\n","\n","def prepare_data():\n","\n","    # generate 2D classification dataset\n","\n","    X,y = make_blobs(n_samples=1000, centers=3,\n","                     \n","                     n_features=2, cluster_std=2,\n","\n","                     random_state=2)\n","    \n","    # one hot encode output variable\n","\n","    y = to_categorical(y)\n","\n","    # split into train and test\n","\n","    n_train = 500\n","\n","    train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","    train_y, test_y = y[:n_train], y[n_train:]\n","\n","    return train_X, train_y, test_X, test_y\n","\n","\n","\n","\n","# fit a model and plot learning curve\n","\n","\n","def fit_model(train_X, train_y, test_X, test_y, optimizer):\n","\n","    # define model\n","\n","    model = Sequential()\n","\n","    model.add(Dense(50, input_dim=2, activation='relu', \n","                    \n","                    kernel_initializer='he_uniform'))\n","    \n","    model.add(Dense(3, activation='softmax'))\n","\n","    # compile model\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  \n","                  optimizer=optimizer, metrics=['accuracy'])\n","    \n","    # fit model\n","\n","    history = model.fit(train_X, train_y, \n","                        \n","                        validation_data=(test_X, test_y),\n","                        \n","                        epochs=200, \n","                        \n","                        verbose=0)\n","    \n","    # plot learning curves\n","\n","    plt.plot(history.history['accuracy'], label='train')\n","\n","    plt.plot(history.history['val_accuracy'], label='test')\n","\n","    plt.title('opt=' + optimizer, pad=-80)\n","\n","\n","\n","# prepare dataset\n","\n","train_X, train_y, test_X, test_y = prepare_data()\n","\n","\n","\n","# create learning curves for different optimizers\n","\n","optimizers = ['sgd', 'rmsprop', 'adagrad', 'adam']\n","\n","\n","for i in range(len(optimizers)):\n","\n","    # Determine the plot number\n","\n","    plot_no = 220 + (i+1)\n","\n","    plt.subplot(plot_no)\n","\n","    # fit model and plot learning curves for an optimizer\n","\n","    fit_model(train_X, train_y, \n","              \n","              test_X, test_y, optimizers[i])\n","    \n","\n","\n","\n","# show leaning curves\n","\n","\n","plt.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uKIFHjO16u8H"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","6.3.1 \n","\n","Regression Predictive Modelling Problem\n","\n","A.\n","\n","Example of generating samples and plotting their distribution for the regression problem\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"eSRNDP1B6vo6"},"source":["# regression predictive modelling problem\n","\n","from sklearn.datasets import make_regression\n","from matplotlib import pyplot as plt\n","\n","# Generate regression dataset\n","\n","X, y = make_regression(n_samples=1000, \n","                       \n","                       n_features=20,\n","                       \n","                       noise=0.1, random_state=1)\n","\n","\n","# histograms of input variables\n","\n","plt.subplot(211)\n","\n","plt.hist(X[:, 0])\n","\n","plt.subplot(212)\n","\n","plt.hist(X[:, 1])\n","\n","plt.show()\n","\n","\n","# histogram of target variable\n","\n","plt.hist(y)\n","\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hzqStwmuWj6V"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","6.3.2\n","\n","Multilayer Perceptron with Unscaled Data\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"vw5HZz3KWkUM"},"source":["# Multi-Layer Perceptron with unscaled data\n","# for the regression problem\n","\n","from sklearn.datasets import make_regression\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# Generate regression dataset\n","\n","X, y = make_regression(n_samples=1000, \n","                       \n","                       n_features=20,\n","                       \n","                       noise=0.1, random_state=1)\n","\n","\n","# split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(25, input_dim=20, activation='relu', \n","                    \n","                kernel_initializer='he_uniform'))\n","    \n","model.add(Dense(1, activation='linear'))\n","\n","# compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='mean_squared_error',\n","                  \n","                optimizer=opt)\n","\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y,\n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=100, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","train_mse = model.evaluate(train_X, train_y, verbose=0)\n","\n","test_mse = model.evaluate(test_X, test_y, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n","\n","\n","\n","\n","# plot loss during training\n","\n","plt.title('Mean Squared Error')\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KCKYauh7dNUz"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","6.3.3\n","\n","Multi-layer Perceptron with Scaled Output Variables\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"d3TS6_JAdOF0"},"source":["# Multi-Layer Perceptron with Scaled Outputs\n","# for the regression problem\n","\n","from sklearn.datasets import make_regression\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# Generate regression dataset\n","\n","X, y = make_regression(n_samples=1000, \n","                       \n","                       n_features=20,\n","                       \n","                       noise=0.1, random_state=1)\n","\n","\n","# split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","# Reshape 1D arrays into 2D arrays\n","\n","train_y = train_y.reshape(len(train_y), 1)\n","\n","test_y = test_y.reshape(len(train_y), 1)\n","\n","\n","\n","# create scaler\n","\n","scaler = StandardScaler()\n","\n","\n","\n","# fit scaler on training dataset\n","\n","scaler.fit(train_y)\n","\n","\n","\n","# transform training dataset\n","\n","train_y = scaler.transform(train_y)\n","\n","\n","\n","# transform test dataset\n","\n","test_y = scaler.transform(test_y)\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(25, input_dim=20, activation='relu', \n","                    \n","                kernel_initializer='he_uniform'))\n","    \n","model.add(Dense(1, activation='linear'))\n","\n","# compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='mean_squared_error',\n","                  \n","                optimizer=opt)\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y,\n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=100, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","train_mse = model.evaluate(train_X, train_y, verbose=0)\n","\n","test_mse = model.evaluate(test_X, test_y, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n","\n","\n","\n","\n","# plot loss during training\n","\n","plt.title('Mean Squared Error Loss')\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EUiUVFYtcV-p"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","6.3.4\n","\n","Multi-Layer Perceptron with Scaled Input Variables\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"wA5rvE0acWbl"},"source":["# compare scaling methods for Multi-Layer Perceptron\n","# inputs on regression problem\n","\n","from sklearn.datasets import make_regression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","from numpy import mean\n","from numpy import std\n","\n","\n","\n","# Prepare dataset with input and output scalers,\n","# can be none\n","\n","def get_dataset(input_scaler, output_scaler):\n","\n","    # generate dataset\n","\n","    X, y = make_regression(n_samples=1000, \n","                           \n","                           n_features=20,\n","                           \n","                           noise=0.1, \n","                           \n","                           random_state=1)\n","    \n","    # split into train and test set\n","\n","    n_train = 500\n","\n","    train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","    train_y, test_y = y[:n_train], y[n_train:]\n","\n","    # scale inputs\n","\n","    if input_scaler is not None:\n","\n","        # fit scaler\n","\n","        input_scaler.fit(train_X)\n","\n","        # tranform training dataset\n","\n","        train_X = input_scaler.transform(train_X)\n","\n","        # transform test dataset\n","\n","        test_X = input_scaler.transform(test_X)\n","\n","\n","    if output_scaler is not None:\n","\n","        # reshape 1D arrays into 2D arrays\n","\n","        train_y = train_y.reshape(len(train_y), 1)\n","\n","        test_y = test_y.reshape(len(train_y), 1)\n","\n","        # fit scaler on training dataset\n","\n","        output_scaler.fit(train_y)\n","\n","        # transform training dataset\n","\n","        train_y = output_scaler.transform(train_y)\n","\n","        # transform test dataset\n","\n","        test_y = output_scaler.transform(test_y)\n","    \n","    return train_X, train_y, test_X, test_y\n","\n","\n","\n","# fit and evaluate mean squared error (mse)\n","# of model on test set\n","\n","def evaluate_model(train_X, train_y, test_X, test_y):\n","\n","    # define model\n","\n","    model = Sequential()\n","\n","    model.add(Dense(25, input_dim=20, activation='relu', \n","                    \n","                    kernel_initializer='he_uniform'))\n","    \n","    model.add(Dense(1, activation='linear'))\n","\n","    # compile model\n","\n","    opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","    model.compile(loss='mean_squared_error',\n","                  \n","                    optimizer=opt)\n","    \n","    # fit model\n","\n","    model.fit(train_X, train_y, epochs=100,\n","              \n","              verbose=0)\n","    \n","    # evaluate the model\n","\n","    test_mse = model.evaluate(test_X, test_y, verbose=0)\n","\n","    return test_mse\n","\n","\n","\n","\n","# evaluate model multiple times with given input and output scalers\n","\n","def repeated_evaluation(input_scaler, output_scaler, n_repeats=30):\n","\n","    # get dataset\n","\n","    train_X, train_y, test_X, test_y  = get_dataset(input_scaler, output_scaler)\n","\n","    # repeated evaluation of model\n","\n","    results = list()\n","\n","    for _ in range(n_repeats):\n","\n","        test_mse = evaluate_model(train_X, train_y, test_X, test_y)\n","\n","        print('>%.3f' % test_mse)\n","\n","        results.append(test_mse)\n","\n","    return results\n","\n","\n","\n","# unscaled inputs\n","\n","results_unscaled_inputs = repeated_evaluation(None,\n","                                              \n","                                              StandardScaler())\n","\n","# normalized inputs\n","\n","results_normalized_inputs = repeated_evaluation(MinMaxScaler(),\n","                                                \n","                                                StandardScaler())\n","\n","# standardized inputs\n","\n","results_standardized_inputs = repeated_evaluation(StandardScaler(),\n","                                                  \n","                                                  StandardScaler())\n","\n","\n","\n","# Summarize results\n","\n","print('Unscaled: %.3f (%.3f)' % (mean(results_unscaled_inputs),\n","                                 \n","                                 std(results_unscaled_inputs)))\n","\n","print('Normalized: %.3f (%.3f)' % (mean(results_normalized_inputs),\n","                                 \n","                                 std(results_normalized_inputs)))\n","\n","print('Standardized: %.3f (%.3f)' % (mean(results_standardized_inputs),\n","                                 \n","                                 std(results_standardized_inputs)))\n","\n","\n","\n","# plot results\n","\n","results = [results_unscaled_inputs, \n","           \n","           results_normalized_inputs,\n","           \n","           results_standardized_inputs]\n","\n","labels = ['unscaled', 'normalized', 'standardized']\n","\n","plt. boxplot(results, labels=labels)\n","\n","plt.show\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sOm6EGOamnny"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","7.3.2 \n","\n","Multi-Layer Perceptron Model\n","\n","\n","--\n","\n","Multi-Layer Perceptron with Tanh for the two circles Classification Problem\n","\n","--\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"3pZAGuJfmoHd"},"source":["# Multi-Layer Perceptron with Tanh for the \n","#two circles Classification Problem\n","\n","from sklearn.datasets import make_circles\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.initializers import RandomUniform\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2D classification dataset\n","\n","X, y = make_circles(n_samples=1000, noise=0.1, \n","                    \n","                    random_state=1)\n","\n","\n","\n","# scale input data into [-1, 1]\n","\n","scaler = MinMaxScaler(feature_range=(-1, 1))\n","\n","X = scaler.fit_transform(X)\n","\n","\n","\n","# Split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","init = RandomUniform(minval=0, maxval=1)\n","\n","model.add(Dense(5, input_dim=2, activation='tanh',\n","                \n","                kernel_initializer=init))\n","\n","model.add(Dense(1, activation='sigmoid',\n","                \n","                kernel_initializer=init))\n","\n","\n","\n","# compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer=opt, metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=500, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","\n","_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","\n","\n","# plot accuracy learning curves\n","\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yKHHODWRhS2Y"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","7.3.3\n","\n","Deeper MLP Model\n","\n","--\n","\n","Deeper MLP with tanh for the two circles classification problem\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"k_Day1JQhTIe"},"source":["# Deeper Multi-Layer Perceptron with Tanh for the \n","#two circles Classification Problem \n","\n","from sklearn.datasets import make_circles\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.initializers import RandomUniform\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2D classification dataset\n","\n","X, y = make_circles(n_samples=1000, noise=0.1, \n","                    \n","                    random_state=1)\n","\n","\n","\n","# scale input data into [-1, 1]\n","\n","scaler = MinMaxScaler(feature_range=(-1, 1))\n","\n","X = scaler.fit_transform(X)\n","\n","\n","\n","# Split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","init = RandomUniform(minval=0, maxval=1)\n","\n","model.add(Dense(5, input_dim=2, activation='tanh',\n","                \n","                kernel_initializer=init))\n","\n","model.add(Dense(5, activation='tanh', kernel_initializer=init))\n","model.add(Dense(5, activation='tanh', kernel_initializer=init))\n","model.add(Dense(5, activation='tanh', kernel_initializer=init))\n","model.add(Dense(5, activation='tanh', kernel_initializer=init))\n","\n","\n","model.add(Dense(1, activation='sigmoid',\n","                \n","                kernel_initializer=init))\n","\n","\n","\n","\n","\n","# compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer=opt, metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=500, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","\n","_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","\n","\n","# plot accuracy learning curves\n","\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jcl1A0_rpHCK"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","7.3.4\n","\n","Deeper MLP Model with ReLU\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"gXNMVfZupHS1"},"source":["# Deeper Multi-Layer Perceptron with ReLU for the \n","#two circles Classification Problem (5 hidden Layers)\n","\n","from sklearn.datasets import make_circles\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2D classification dataset\n","\n","X, y = make_circles(n_samples=1000, noise=0.1, \n","                    \n","                    random_state=1)\n","\n","\n","\n","# scale input data into [-1, 1]\n","\n","scaler = MinMaxScaler(feature_range=(-1, 1))\n","\n","X = scaler.fit_transform(X)\n","\n","\n","\n","# Split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(5, input_dim=2, activation='relu',\n","                \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(5, activation='relu', \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(5, activation='relu', \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(5, activation='relu', \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(5, activation='relu', \n","                kernel_initializer='he_uniform'))\n","\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","\n","\n","\n","\n","# compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer=opt, metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=500, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","\n","_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","\n","\n","# plot accuracy learning curves\n","\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"02dExXW_roFD"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","8.3.1 \n","\n","Regression Predictive Modelling Problem\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"0nbVWE4aroTK"},"source":["# Visualize the distribution of the target variable\n","\n","# Regression Predictive Modelling Problem\n","\n","from sklearn.datasets import make_regression\n","from matplotlib import pyplot as plt\n","\n","# generate regression dataset\n","\n","X, y = make_regression(n_samples=1000, n_features=20, \n","                       \n","                       noise=0.1, random_state=1)\n","\n","# histogram of target variable\n","\n","plt.subplot(121)\n","\n","plt.hist(y)\n","\n","# box plot of target variable\n","\n","plt.subplot(122)\n","\n","plt.boxplot(y)\n","\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lb1cFe09t7gF"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","8.3.2 \n","\n","Multi-Layer Perceptron with exploding gradients\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"YlKwgFIpt7xi"},"source":["# MLP with unscaled data for the regression problem\n","\n","from sklearn.datasets import make_regression\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate regression dataset\n","\n","X, y = make_regression(n_samples=1000, n_features=20, \n","                       \n","                       noise=0.1, random_state=1)\n","\n","\n","\n","# Split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(25, input_dim=20, activation='relu',\n","                \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(1, activation='linear'))\n","\n","\n","\n","# compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='mean_squared_error', \n","              \n","              optimizer=opt, metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=100, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","\n","_, train_mse = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_mse = model.evaluate(test_X, test_y, verbose=0)\n","\n","\n","print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n","\n","\n","# plot loss during training\n","\n","plt.title('Mean Squared Error')\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v8F-TIvhv6WY"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","8.3.3 \n","\n","Multi-Layer Perceptron with Gradient Norm Scaling\n","\n","--\n","\n","MLP with Unscaled data for the regression problem with gradient norm scaling\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"FF0TBWNJv6pF"},"source":["# MLP with Unscaled data for the regression problem \n","# with gradient norm scaling\n","\n","from sklearn.datasets import make_regression\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate regression dataset\n","\n","X, y = make_regression(n_samples=1000, n_features=20, \n","                       \n","                       noise=0.1, random_state=1)\n","\n","\n","\n","# Split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(25, input_dim=20, activation='relu',\n","                \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(1, activation='linear'))\n","\n","\n","\n","# compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9, clipnorm=1.0)\n","\n","model.compile(loss='mean_squared_error', \n","              \n","              optimizer=opt, metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=100, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","\n","_, train_mse = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_mse = model.evaluate(test_X, test_y, verbose=0)\n","\n","\n","print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n","\n","\n","# plot loss during training\n","\n","plt.title('Mean Squared Error')\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r9PirbYDz8sl"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","8.3.4 \n","\n","Multi-Layer Perceptron with Gradient Value Clipping\n","\n","--\n","\n","MLP with Unscaled data for the regression problem with gradient clipping\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"687lgVwJz9D5"},"source":["# MLP with Unscaled data for the \n","# regression problem with gradient clipping\n","\n","from sklearn.datasets import make_regression\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate regression dataset\n","\n","X, y = make_regression(n_samples=1000, n_features=20, \n","                       \n","                       noise=0.1, random_state=1)\n","\n","\n","\n","# Split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(25, input_dim=20, activation='relu',\n","                \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(1, activation='linear'))\n","\n","\n","\n","# compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9, clipvalue=5.0)\n","\n","model.compile(loss='mean_squared_error', \n","              \n","              optimizer=opt, metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=100, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","\n","_, train_mse = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_mse = model.evaluate(test_X, test_y, verbose=0)\n","\n","\n","print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n","\n","\n","# plot loss during training\n","\n","plt.title('Mean Squared Error')\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qaYR2PT90koa"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","9.3.2 \n","\n","Multi-Layer Perceptron Model\n","\n","--\n","\n","MLP for the two circles problem\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"F4V3XjnE0k5r"},"source":["# MLP for the two circles problem\n","\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2D classification dataset\n","\n","X, y = make_circles(n_samples=1000, \n","                       \n","                       noise=0.1, random_state=1)\n","\n","\n","\n","# Split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(50, input_dim=2, activation='relu',\n","                \n","                kernel_initializer='he_uniform'))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","\n","\n","# compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer=opt, metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=100, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","\n","_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","\n","\n","# plot accuracy learning curves\n","\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","plt.show()\n","\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xQ0DIXHUwg_t"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","9.3.2 \n","\n","Multi-Layer Perceptron with Batch Normalization\n","\n","--\n","\n","A.\n","\n","MLP for the two circles problem with batchnorm after activation function\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"jDcgy9XwwhOC"},"source":["# MLP for the two circles problem with \n","# BatchNorm AFTER the activation Function\n","\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2D classification dataset\n","\n","X, y = make_circles(n_samples=1000, \n","                       \n","                       noise=0.1, random_state=1)\n","\n","\n","\n","# Split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(50, input_dim=2, activation='relu',\n","                \n","                kernel_initializer='he_uniform'))\n","\n","model.add(BatchNormalization())\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","\n","\n","# compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer=opt, metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=100, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","\n","_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","\n","\n","# plot accuracy learning curves\n","\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","plt.show()\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kwdblVOZ0YBh"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","9.3.2 \n","\n","Multi-Layer Perceptron with Batch Normalization\n","\n","--\n","\n","B.\n","\n","MLP for the two circles problem with batchnorm BEFORE activation function\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"8MIxAf1t0j9-"},"source":["# MLP for the two circles problem with \n","# BatchNorm BEFORE the activation Function\n","\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2D classification dataset\n","\n","X, y = make_circles(n_samples=1000, \n","                       \n","                       noise=0.1, random_state=1)\n","\n","\n","\n","# Split into train and test\n","\n","n_train = 500\n","\n","train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(50, input_dim=2, activation='relu',\n","                \n","                kernel_initializer='he_uniform'))\n","\n","model.add(BatchNormalization())\n","\n","model.add(Activation('relu'))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","\n","\n","# compile model\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer=opt, metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(train_X, train_y, \n","                    \n","                    validation_data=(test_X, test_y),\n","                    \n","                    epochs=100, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","\n","_, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","_, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","\n","\n","# plot accuracy learning curves\n","\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","plt.show()\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kl1vnXj-1-lQ"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","10.2.2 \n","\n","Supervised Greedy Layer-Wise Pre-Training\n","--\n","\n","Supervised Greedy Layer-Wise Pre-Training for blobs Classification Problem\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"rbm6rJWk1-8Q"},"source":["# Supervised Greedy Layer-Wise Pre-Training \n","# for blobs Classification Problem\n","\n","from sklearn.datasets import make_blobs\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.utils import to_categorical\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# prepare train and test dataset\n","\n","\n","def prepare_data():\n","\n","    # generate 2D classification dataset\n","\n","    X,y = make_blobs(n_samples=1000, centers=3,\n","                     \n","                     n_features=2, cluster_std=2,\n","\n","                     random_state=2)\n","    \n","    # one hot encode output variable\n","\n","    y = to_categorical(y)\n","\n","    # split into train and test\n","\n","    n_train = 500\n","\n","    train_X, test_X = X[:n_train, :], X[n_train:, :]\n","\n","    train_y, test_y = y[:n_train], y[n_train:]\n","\n","    return train_X, test_X, train_y, test_y\n","\n","\n","\n","# define and fit the base model\n","\n","def get_base_model(train_X, train_y):\n","\n","    # define model\n","\n","    model = Sequential()\n","\n","    model.add(Dense(10, input_dim=2, activation='relu', \n","                    \n","                    kernel_initializer='he_uniform'))\n","    \n","    model.add(Dense(3, activation='softmax'))\n","\n","    # compile model\n","\n","    opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","    model.compile(loss='categorical_crossentropy', \n","                  \n","                  optimizer=opt, metrics=['accuracy'])\n","    \n","    # fit model\n","\n","    model.fit(train_X, train_y, epochs=100, verbose=0)\n","\n","    return model\n","\n","\n","\n","\n","\n","# evaluate a fit model\n","\n","def evaluate_model(model, train_X, test_X, train_y, test_y):\n","\n","    _, train_acc = model.evaluate(train_X, train_y, verbose=0)\n","\n","    _, test_acc = model.evaluate(test_X, test_y, verbose=0)\n","\n","    return train_acc, test_acc\n","\n","\n","\n","\n","# add one new layer and retrain only the new layer\n","\n","def add_layer(model, train_X, train_y):\n","\n","    # remember the current output layer\n","\n","    output_layer = model.layers[-1]\n","\n","    # remove the output layer\n","\n","    model.pop()\n","\n","    # mark all remaining layers as non-trainable\n","\n","    for layer in model.layers:\n","\n","        layer.trainable = False\n","\n","    # add a new hidden layer\n","\n","    model.add(Dense(10, activation='relu',\n","                    \n","                    kernel_initializer='he_uniform'))\n","    \n","    # re-add the output layer\n","\n","    model.add(output_layer)\n","\n","    # fit model\n","\n","    model.fit(train_X, train_y, epochs=100, verbose=0)\n","\n","\n","\n","\n","\n","# prepare the data\n","\n","train_X, test_X, train_y, test_y = prepare_data()\n","\n","\n","\n","# get the base model\n","\n","model = get_base_model(train_X, train_y)\n","\n","\n","\n","# evaluate the base model\n","\n","scores = dict()\n","\n","train_acc, test_acc = evaluate_model(model, \n","                                     \n","                                     train_X, test_X,\n","                                     \n","                                     train_y, test_y)\n","\n","print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n","\n","scores[len(model.layers)] = (train_acc, test_acc)\n","\n","\n","\n","\n","\n","\n","# add layers and evaluate the updated model\n","\n","n_layers = 10\n","\n","for i in range(n_layers):\n","\n","    # add layer\n","\n","    add_layer(model, train_X, train_y)\n","\n","    # evaluate model\n","\n","    train_acc, test_acc = evaluate_model(model, \n","                                         \n","                                         train_X, test_X, \n","                                         \n","                                         train_y, test_y)\n","    \n","    print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n","\n","    # store scores for plotting\n","\n","    scores[len(model.layers)] = (train_acc, test_acc)\n","\n","\n","\n","\n","\n","# plot number of added layers vs accuracy\n","\n","plt.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.')\n","\n","plt.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.')\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o4tkAMTpQnaV"},"source":["--\n","\n","\n","--\n","\n","--\n","\n","---\n","\n","NOTE\n","\n","Henceforth, go to Jason Brownlee's website and copy the code, then paste it here and run\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xTchkLQ_RPqk"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","13.3.2 \n","\n","Overfit Multi-Layer Perceptron Model\n","--\n","\n","Overfit Multi-Layer Perceptron for the moons dataset\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"PXJ7YbmzQoEQ"},"source":["# overfit mlp for the moons dataset\n","\n","from sklearn.datasets import make_moons\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(500, input_dim=2, activation='relu'))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer='adam', metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, epochs=4000, \n","                    \n","                    validation_data=(testX, testy), verbose=0)\n","\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","\n","\n","# plot accuracy learning curves\n","\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","plt.legend()\n","\n","\n","\n","plt.show()\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hqX3JIM4zk5r"},"source":["--\n","\n","--\n","\n","--\n","\n","--\n","\n","---\n","\n","13.3.3 \n","\n","Multi-Layer Perceptron Model with Weight Regularisation\n","--\n","\n","Multi-Layer Perceptron Model with Weight Regularisation for the moons dataset\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"vqLMDWVmzlPs"},"source":["# mlp with weight regularization for the moons dataset\n","\n","from sklearn.datasets import make_moons\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.regularizers import l2\n","import matplotlib.pyplot as plt\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(500, input_dim=2, \n","                \n","                activation='relu', \n","                \n","                kernel_regularizer=l2(0.001)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer='adam', metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, epochs=4000, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BTwhaFN11aly"},"source":["--\n","\n","--\n","\n","---\n","\n","13.3.4 \n","\n","Grid Search Regularization Hyperparameter\n","--\n","\n","rid Search Regularization values for moons dataset\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"f5-Iv1YF1a6s"},"source":["# grid search regularization values for moons dataset\n","\n","from sklearn.datasets import make_moons\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.regularizers import l2\n","from matplotlib import pyplot\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# grid search values\n","\n","values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n","\n","all_train, all_test = list(), list()\n","\n","\n","\n","for param in values:\n","\n","    # define model\n","\n","    model = Sequential()\n","    \n","    model.add(Dense(500, input_dim=2, \n","                    \n","                    activation='relu', \n","                    \n","                    kernel_regularizer=l2(param)))\n","        \n","    model.add(Dense(1, activation='sigmoid'))\n","\n","    model.compile(loss='binary_crossentropy', \n","                  \n","                  optimizer='adam', metrics=['accuracy'])\n","    \n","    # fit model\n","\n","    model.fit(trainX, trainy, epochs=4000, verbose=0)\n","    \n","    # evaluate the model\n","\n","    _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","    \n","    _, test_acc = model.evaluate(testX, testy, verbose=0)\n","    \n","    print('Param: %f, Train: %.3f, Test: %.3f' % (param, train_acc, test_acc))\n","        \n","    all_train.append(train_acc)\n","        \n","    all_test.append(test_acc)\n","\n","\n","\n","\n","# plot train and test means\n","\n","pyplot.semilogx(values, all_train, label='train', marker='o')\n","\n","pyplot.semilogx(values, all_test, label='test', marker='o')\n","\n","pyplot.legend()\n","\n","pyplot.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6JMmAw8Q8SMy"},"source":["--\n","\n","--\n","\n","---\n","\n","14.3.2 \n","\n","Overfit Multi-Layer Perceptron\n","--\n","\n","MLP overfit on the two-circles dataset\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"cj_SbhjK8S5a"},"source":["# mlp overfit on the two circles dataset\n","\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from matplotlib import pyplot as plt\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(500, input_dim=2, activation='relu'))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer='adam', metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","\n","                    epochs=4000, verbose=0)\n","\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IwgFGfTl5O4V"},"source":["--\n","\n","--\n","\n","---\n","\n","14.3.3 \n","\n","Overfit Multi-Layer Perceptron with\n","\n","Activation Regularization\n","--\n","\n","MLP overfit on the two-circles dataset with activation regularisation BEFORE activation\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"XpL8vKQk5PK1"},"source":["# mlp overfit on the two circles dataset\n","# with ACTIVATION REGULARIZATION before ACTIVATION\n","\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.regularizers import l1\n","from tensorflow.keras.layers import Activation\n","from matplotlib import pyplot as plt\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(500, input_dim=2, activation='linear',\n","                \n","                activity_regularizer=l1(0.0001)))\n","\n","model.add(Activation('relu'))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer='adam', metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","\n","                    epochs=4000, verbose=0)\n","\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0LwcugLc6hbS"},"source":["--\n","\n","--\n","\n","---\n","\n","14.3.3 B\n","\n","Overfit Multi-Layer Perceptron with\n","\n","Activation Regularization\n","--\n","\n","MLP overfit on the two-circles dataset with activation regularisation AFTER activation\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"FVjvbjK66hvk"},"source":["# mlp overfit on the two circles dataset\n","# with ACTIVATION REGULARIZATION after ACTIVATION\n","\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.regularizers import l1\n","from tensorflow.keras.layers import Activation\n","from matplotlib import pyplot as plt\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(500, input_dim=2, activation='relu',\n","                \n","                activity_regularizer=l1(0.0001)))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer='adam', metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","\n","                    epochs=4000, verbose=0)\n","\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dhy9hi6v-HwR"},"source":["--\n","\n","--\n","\n","---\n","\n","15.3.2\n","\n","Overfit Multi-Layer Perceptron \n","--\n","\n","MLP overfit on the moons dataset \n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"wz9PtJ5H-IBW"},"source":["# mlp overfit on the moons dataset\n","\n","from sklearn.datasets import make_moons\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(500, input_dim=2, activation='relu'))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer='adam', metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=4000, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zkKYfxHZ3fq-"},"source":["--\n","\n","--\n","\n","---\n","\n","15.3.3\n","\n","Overfit Multi-Layer Perceptron \n","\n","with Weight Constraint\n","--\n","\n","MLP overfit on the moons dataset with a unit form constraint\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"7MfYaZyH3gHi"},"source":["# mlp overfit on the moons dataset with a unit norm constraint\n","\n","from sklearn.datasets import make_moons\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.constraints import unit_norm\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(500, input_dim=2, \n","                \n","                activation='relu', \n","                \n","                kernel_constraint=unit_norm()))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy',\n","               \n","              optimizer='adam',\n","\n","              metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=4000, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qOJxfI1U6NOt"},"source":["--\n","\n","--\n","\n","---\n","\n","16.3.2\n","\n","Overfit Multi-Layer Perceptron \n","\n","\n","--\n","\n","MLP overfit on the two circles dataset \n","\n","\n","---\n","Please CHECK OUT THIS POWERFUL DROP OUT EXAMPLE\n","\n","https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"5mujXqlo6Ngd"},"source":["# mlp overfit on the two circles dataset\n","\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(500, input_dim=2, activation='relu'))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer='adam', metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=4000, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mp9phAFGtiyv"},"source":["--\n","\n","--\n","\n","---\n","\n","16.3.3\n","\n","Multi-Layer Perceptron with Dropout Regularization\n","\n","\n","--\n","\n","MLP with DROP OUT on the two circles dataset \n","\n","\n","---\n","Please CHECK OUT THIS POWERFUL DROP OUT EXAMPLE\n","\n","https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"X6mOxbhstjEr"},"source":["# mlp overfit on the two circles dataset\n","\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dropout\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(500, input_dim=2, activation='relu'))\n","\n","model.add(Dropout(0.4))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer='adam', metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=4000, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i971Ge8mvQeZ"},"source":["--\n","\n","--\n","\n","---\n","\n","17.3.2\n","\n","Overfit Multi-Layer Perceptron\n","--\n","\n","MLP overfit of the two circles dataset\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"bBOZCdUavQw5"},"source":["# mlp overfit on the two circles dataset\n","\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","trainX, testX = X[: n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","model = Sequential()\n","\n","model.add(Dense(500, input_dim=2, activation='relu'))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer='adam', \n","              \n","              metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=4000, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M6znBE6ZG-KV"},"source":["--\n","\n","--\n","\n","---\n","\n","17.3.3\n","\n","Multi-Layer Perceptron with input layer noise\n","\n","--\n","\n","MLP overfit of the two circles dataset with input noise\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"OHTcRB1ZG-u2"},"source":["# mlp overfit on the two circles dataset with input noise\n","\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import GaussianNoise\n","from matplotlib import pyplot as plt\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(GaussianNoise(0.01, input_shape=(2,)))\n","\n","model.add(Dense(500, activation='relu'))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer='adam', \n","              \n","              metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=4000, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dQjdblWoOSDb"},"source":["--\n","\n","--\n","\n","---\n","\n","17.3.4\n","\n","Multi-Layer Perceptron with Hidden Layer Noise\n","\n","--\n","\n","MLP overfit of the two circles dataset with Hidden Layer Noise\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"jl-eGamSOSkH"},"source":["# mlp overfit on the two circles dataset with \n","# hidden layer noise\n","\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import GaussianNoise\n","from matplotlib import pyplot as plt\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(500, input_dim=2))\n","\n","model.add(GaussianNoise(0.1))\n","\n","model.add(Activation('relu'))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer='adam', \n","              \n","              metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=4000, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"btfa9wx8QF2d"},"source":["--\n","\n","--\n","\n","---\n","\n","17.3.4 B\n","\n","Alternative\n","\n","--\n","\n","MLP Overfit on the two circles dataset with hidden layer noise (alternative)\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"Yg9tkaJbQGHl"},"source":["# mlp overfit on the two circles dataset with \n","# hidden layer noise (alternative)\n","\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import GaussianNoise\n","from matplotlib import pyplot as plt\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(500, input_dim=2, activation='relu'))\n","\n","model.add(GaussianNoise(0.1))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer='adam', \n","              \n","              metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=4000, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YwYv_ZLxTVFG"},"source":["--\n","\n","--\n","\n","---\n","\n","18.3.2\n","\n","Overfit Multi-Layer Perceptron \n","\n","--\n","\n","MLP overfit on the moons dataset \n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"gEoKCMa6TVV8"},"source":["# mlp overfit on the moons dataset\n","\n","from sklearn.datasets import make_moons\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(500, input_dim=2, activation='relu'))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer='adam', \n","              \n","              metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=4000, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CzFqHYm_PEoy"},"source":["--\n","\n","--\n","\n","---\n","\n","18.3.3\n","\n","Overfit MLP with Early Stopping\n","\n","--\n","\n","MLP overfit on the moons dataset with SIMPLE EARLY STOPPING\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"gU2xwIc8PE6L"},"source":["# mlp overfit on the moons dataset with ''SIMPLE'' early stopping\n","\n","\n","\n","# NOTE THAT as opposed to PAGE 358 - 359 of \n","\n","# Better Deep Learning, a patience of \n","\n","#1 (negligible, almost zero) has been used to obtain \n","\n","# similar results.\n","\n","\n","\n","\n","\n","from sklearn.datasets import make_moons\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.callbacks import EarlyStopping\n","from matplotlib import pyplot as plt\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(500, input_dim=2, activation='relu'))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer='adam', metrics=['accuracy'])\n","\n","\n","\n","# patient early stopping\n","\n","es = EarlyStopping(monitor='val_loss', \n","                   \n","                   mode='min', \n","                   \n","                   verbose=1, patience=1)\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=4000, \n","                    \n","                    verbose=0, callbacks=[es])\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AjY-hPQIQOZz"},"source":["--\n","\n","--\n","\n","---\n","\n","18.3.3\n","\n","B. \n","\n","Overfit MLP with Early Stopping\n","\n","--\n","\n","MLP overfit on the moons dataset with ...PATIENT... EARLY STOPPING\n","\n","FIT IN THE CHANGES (add the patience argument)\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"eshFvsh3QOpU"},"source":["# mlp overfit on the moons dataset with patient early stopping\n","\n","from sklearn.datasets import make_moons\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.callbacks import EarlyStopping\n","from matplotlib import pyplot as plt\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(500, input_dim=2, activation='relu'))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer='adam', metrics=['accuracy'])\n","\n","\n","\n","# patient early stopping\n","\n","es = EarlyStopping(monitor='val_loss', \n","                   \n","                   mode='min', \n","                   \n","                   verbose=1, patience=200)\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=4000, \n","                    \n","                    verbose=0, callbacks=[es])\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8gA4zCuPW5J_"},"source":["--\n","\n","--\n","\n","---\n","\n","18.3.3\n","\n","C. \n","\n","Overfit MLP with Early Stopping\n","\n","--\n","\n","MLP overfit on the moons dataset with ...PATIENT... EARLY STOPPING and MODEL CHECKPOINTING\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"cALWlfjCW5ZK"},"source":["# mlp overfit on the moons dataset with \n","# patient early stopping and model checkpointing\n","\n","from sklearn.datasets import make_moons\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.models import load_model\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 30\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(500, input_dim=2, activation='relu'))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', \n","              \n","              optimizer='adam', \n","              \n","              metrics=['accuracy'])\n","\n","\n","\n","# simple early stopping\n","\n","es = EarlyStopping(monitor='val_loss', \n","                   \n","                   mode='min', \n","                   \n","                   verbose=1, patience=200)\n","\n","\n","mc = ModelCheckpoint('best_model.h5', \n","                     \n","                     monitor='val_accuracy', \n","                     \n","                     mode='max', verbose=1, \n","                     \n","                     save_best_only=True)\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=4000, verbose=0, \n","                    \n","                    callbacks=[es, mc])\n","\n","\n","\n","# load the saved model\n","\n","saved_model = load_model('best_model.h5')\n","\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = saved_model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = saved_model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ydyVffGZovU"},"source":["--\n","\n","--\n","\n","---\n","\n","20.3.2\n"," \n","\n","MLP Model for Multiclass Classification\n","\n","--\n","\n","fit high variance MLP on blobs classification problem\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"Bso-TwLAZo9-"},"source":["# fit high variance mlp on blobs classification problem\n","\n","from sklearn.datasets import make_blobs\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=500, centers=3, \n","                  \n","                  n_features=2, \n","                  \n","                  cluster_std=2, random_state=2)\n","\n","y = to_categorical(y)\n","\n","\n","\n","# split into train and test\n","\n","n_train = int(0.3 * X.shape[0])\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(15, input_dim=2, activation='relu'))\n","\n","model.add(Dense(3, activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy', \n","              \n","              optimizer='adam', \n","              \n","              metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=200, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SUVxFDrVv8Gx"},"source":["--\n"," \n","--\n","\n","--\n","\n","--\n","\n","---\n","20.3.3\n","\n","High Variance MLP Model \n","\n","--\n","\n","demonstrate high variance of MLP on blobs classification problem\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"7gi-J2Qwv8yL"},"source":["# demonstrate high variance of mlp model on \n","# blobs classification problem\n","\n","from sklearn.datasets import make_blobs\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from numpy import mean\n","from numpy import std\n","from matplotlib import pyplot\n","\n","\n","\n","# fit and evaluate a neural net model on the dataset\n","\n","def evaluate_model(trainX, trainy, testX, testy):\n","\n","\t# define model\n","\n","\tmodel = Sequential()\n"," \n","\tmodel.add(Dense(15, input_dim=2, activation='relu'))\n"," \n","\tmodel.add(Dense(3, activation='softmax'))\n"," \n","\tmodel.compile(loss='categorical_crossentropy', \n","               \n","               optimizer='adam', metrics=['accuracy'])\n"," \n","\t# fit model\n","\n","\tmodel.fit(trainX, trainy, epochs=200, verbose=0)\n"," \n","\t# evaluate the model\n","\n","\t_, test_acc = model.evaluate(testX, testy, verbose=0)\n"," \n","\treturn test_acc\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=500, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","y = to_categorical(y)\n","\n","\n","\n","\n","# split into train and test\n","\n","n_train = int(0.3 * X.shape[0])\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# repeated evaluation\n","\n","n_repeats = 30\n","\n","scores = list()\n","\n","\n","for _ in range(n_repeats):\n","\n","\tscore = evaluate_model(trainX, trainy, testX, testy)\n"," \n","\tprint('> %.3f' % score)\n"," \n","\tscores.append(score)\n"," \n","\n","\n","# summarize the distribution of scores\n","\n","print('Scores Mean: %.3f, Standard Deviation: %.3f' % (mean(scores), std(scores)))\n","\n","\n","\n","# histogram of distribution\n","\n","pyplot.hist(scores, bins=10)\n","\n","pyplot.show()\n","\n","# boxplot of distribution\n","\n","pyplot.boxplot(scores)\n","\n","pyplot.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B5YC9kjky4BB"},"source":["--\n"," \n","--\n","\n","--\n","\n","--\n","\n","---\n","20.3.4\n","\n","Model Averaging Ensemble \n","\n","--\n","\n","Model Averaging Ensemble and a study of ensemble size on model accuracy\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"srs_7cw9y4QL"},"source":["# model averaging ensemble and a study of ensemble \n","# size on test accuracy\n","\n","from sklearn.datasets import make_blobs\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","import numpy\n","from numpy import array\n","from numpy import argmax\n","from sklearn.metrics import accuracy_score\n","from matplotlib import pyplot\n","\n","\n","\n","# fit model on dataset\n","\n","def fit_model(trainX, trainy):\n","\n","\t# define model\n","\n","\tmodel = Sequential()\n"," \n","\tmodel.add(Dense(15, input_dim=2, activation='relu'))\n"," \n","\tmodel.add(Dense(3, activation='softmax'))\n"," \n","\tmodel.compile(loss='categorical_crossentropy', \n","               \n","               optimizer='adam', metrics=['accuracy'])\n","\t\n","    # fit model\n","\n","\tmodel.fit(trainX, trainy, epochs=200, verbose=0)\n"," \n","\treturn model\n","\n","\n","\n","\n","# make an ensemble prediction for multi-class classification\n","\n","def ensemble_predictions(members, testX):\n","\n","\t# make predictions\n","\n","\tyhats = [model.predict(testX) for model in members]\n","\n","\tyhats = array(yhats)\n"," \n","\t# sum across ensemble members\n","\n","\tsummed = numpy.sum(yhats, axis=0)\n"," \n","\t# argmax across classes\n","\n","\tresult = argmax(summed, axis=1)\n"," \n","\treturn result\n","\n","\n","\n","\n","# evaluate a specific number of members in an ensemble\n","\n","def evaluate_n_members(members, n_members, testX, testy):\n","\n","\t# select a subset of members\n","\n","\tsubset = members[:n_members]\n","\n","\tprint(len(subset))\n"," \n","\t# make prediction\n","\n","\tyhat = ensemble_predictions(subset, testX)\n"," \n","\t# calculate accuracy\n","\n","\treturn accuracy_score(testy, yhat)\n"," \n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=500, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","# split into train and test\n","\n","n_train = int(0.3 * X.shape[0])\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","trainy = to_categorical(trainy)\n","\n","\n","\n","\n","# fit all models\n","\n","n_members = 20\n","\n","members = [fit_model(trainX, trainy) for _ in range(n_members)]\n","\n","\n","\n","\n","# evaluate different numbers of ensembles\n","\n","scores = list()\n","\n","for i in range(1, n_members+1):\n","\n","\tscore = evaluate_n_members(members, i, testX, testy)\n"," \n","\tprint('> %.3f' % score)\n"," \n","\tscores.append(score)\n"," \n","\n","\n","\n","# plot score vs number of ensemble members\n","\n","x_axis = [i for i in range(1, n_members+1)]\n","\n","pyplot.plot(x_axis, scores)\n","\n","pyplot.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UKe5m_IY25og"},"source":["--\n","\n","--\n","\n","---\n","\n","20.3.4\n","\n","Model Averaging Ensemble\n","\n","--\n","\n","Finally, we can update the repeated evaluation experiment to use an ensemble of five models instead of a single model and compare the distribution of scores.\n","\n","The complete example of a repeated evaluated five-member ensemble of the blobs dataset is listed below.\n","\n","---\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"H3yjEd1X254k"},"source":["# repeated evaluation of model averaging \n","# ensemble on blobs dataset\n","\n","from sklearn.datasets import make_blobs\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","import numpy\n","from numpy import array\n","from numpy import argmax\n","from numpy import mean\n","from numpy import std\n","from sklearn.metrics import accuracy_score\n","\n","\n","\n","# fit model on dataset\n","\n","def fit_model(trainX, trainy):\n","\n","\t# define model\n","\n","\tmodel = Sequential()\n"," \n","\tmodel.add(Dense(15, input_dim=2, activation='relu'))\n"," \n","\tmodel.add(Dense(3, activation='softmax'))\n"," \n","\tmodel.compile(loss='categorical_crossentropy', \n","               \n","               optimizer='adam', metrics=['accuracy'])\n"," \n","\t# fit model\n","\n","\tmodel.fit(trainX, trainy, epochs=200, verbose=0)\n"," \n","\treturn model\n","\n","\n","\n","\n","# make an ensemble prediction for multi-class classification\n","\n","def ensemble_predictions(members, testX):\n","\n","\t# make predictions\n","\n","\tyhats = [model.predict(testX) for model in members]\n","\n","\tyhats = array(yhats)\n"," \n","\t# sum across ensemble members\n","\n","\tsummed = numpy.sum(yhats, axis=0)\n"," \n","\t# argmax across classes\n","\n","\tresult = argmax(summed, axis=1)\n"," \n","\treturn result\n","\n","\n","\n","\n","# evaluate ensemble model\n","\n","def evaluate_members(members, testX, testy):\n","\n","\t# make prediction\n","\n","\tyhat = ensemble_predictions(members, testX)\n"," \n","\t# calculate accuracy\n","\n","\treturn accuracy_score(testy, yhat)\n"," \n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=500, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","# split into train and test\n","\n","n_train = int(0.3 * X.shape[0])\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","trainy = to_categorical(trainy)\n","\n","\n","\n","\n","# repeated evaluation\n","\n","n_repeats = 30\n","n_members = 5\n","scores = list()\n","\n","\n","for _ in range(n_repeats):\n","\n","\t# fit all models\n","\n","\tmembers = [fit_model(trainX, trainy) for _ in range(n_members)]\n","\n","\t# evaluate ensemble\n","\n","\tscore = evaluate_members(members, testX, testy)\n"," \n","\tprint('> %.3f' % score)\n"," \n","\tscores.append(score)\n"," \n","\n","\n","\n","# summarize the distribution of scores\n","\n","print('Scores Mean: %.3f, Standard Deviation: %.3f' % (mean(scores), std(scores)))\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BFKHJAYE6X7a"},"source":["--\n"," \n","--\n","\n","--\n","\n","--\n","\n","---\n","21.2.2\n","\n","MLP Model \n","\n","--\n","\n","Develop an MLP for blobs dataset\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"NDi4bpxA6YMY"},"source":["# develop an mlp for blobs dataset\n","\n","from sklearn.datasets import make_blobs\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot as plt\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","print(trainX.shape, testX.shape)\n","\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(25, input_dim=2, activation='relu'))\n","\n","model.add(Dense(3, activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy', \n","              \n","              optimizer='adam', metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=500, verbose=0)\n","\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3QZlwOzMf-N3"},"source":["--\n"," \n","--\n","\n","--\n","\n","--\n","\n","---\n","21.2.3\n","\n","Model Averaging Ensemble\n","\n","--\n","\n","Model Averaging Ensemble for the blobs dataset\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"DNdTLm_af-sQ"},"source":["# model averaging ensemble for the blobs dataset\n","\n","from sklearn.datasets import make_blobs\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot\n","from numpy import mean\n","from numpy import std\n","import numpy\n","from numpy import array\n","from numpy import argmax\n","\n","\n","\n","# fit model on dataset\n","\n","def fit_model(trainX, trainy):\n","\n","\ttrainy_enc = to_categorical(trainy)\n"," \n","\t# define model\n","\n","\tmodel = Sequential()\n"," \n","\tmodel.add(Dense(25, input_dim=2, activation='relu'))\n"," \n","\tmodel.add(Dense(3, activation='softmax'))\n"," \n","\tmodel.compile(loss='categorical_crossentropy', \n","               \n","               optimizer='adam', metrics=['accuracy'])\n"," \n","\t# fit model\n","\n","\tmodel.fit(trainX, trainy_enc, epochs=500, verbose=0)\n"," \n","\treturn model\n","\n","\n","\n","\n","# make an ensemble prediction for multi-class classification\n","\n","def ensemble_predictions(members, testX):\n","\n","\t# make predictions\n","\n","\tyhats = [model.predict(testX) for model in members]\n","\n","\tyhats = array(yhats)\n"," \n","\t# sum across ensemble members\n","\n","\tsummed = numpy.sum(yhats, axis=0)\n"," \n","\t# argmax across classes\n","\n","\tresult = argmax(summed, axis=1)\n"," \n","\treturn result\n","\n","\n","\n","\n","# evaluate a specific number of members in an ensemble\n","\n","def evaluate_n_members(members, n_members, testX, testy):\n","\n","\t# select a subset of members\n","\n","\tsubset = members[:n_members]\n","\n","\t# make prediction\n","\n","\tyhat = ensemble_predictions(subset, testX)\n"," \n","\t# calculate accuracy\n","\n","\treturn accuracy_score(testy, yhat)\n"," \n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","print(trainX.shape, testX.shape)\n","\n","\n","\n","# fit all models\n","\n","n_members = 10\n","\n","members = [fit_model(trainX, trainy) for _ in range(n_members)]\n","\n","\n","\n","\n","# evaluate different numbers of ensembles on hold out set\n","\n","single_scores, ensemble_scores = list(), list()\n","\n","for i in range(1, len(members)+1):\n","\n","    # evaluate model with i members\n","\n","    ensemble_score = evaluate_n_members(members, i, testX, testy)\n","\n","    # evaluate the i'th model standalone\n","\n","    testy_enc = to_categorical(testy)\n","\n","    _, single_score = members[i-1].evaluate(testX, \n","                                        \n","                                        testy_enc, verbose=0)\n","\n","    # summarize this step\n","\n","    print('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n","\n","    ensemble_scores.append(ensemble_score)\n","\n","    single_scores.append(single_score)\n","\n","\n","\n","# summarize average accuracy of a single final model\n","\n","print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))\n","\n","\n","\n","# plot score vs number of ensemble members\n","\n","x_axis = [i for i in range(1, len(members)+1)]\n","\n","pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n","\n","pyplot.plot(x_axis, ensemble_scores, marker='o')\n","\n","pyplot.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eIrfMomijq0y"},"source":["--\n"," \n","--\n","\n","--\n","\n","--\n","\n","---\n","21.2.4\n","\n","Grid Search Weighted Average Ensemble\n","\n","--\n","\n","Grid Search for Coefficients in a Weighted Average Ensemble for the blobs problem\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"dRM1vqpjjrER"},"source":["# grid search for coefficients in a weighted average \n","# ensemble for the blobs problem\n","\n","from sklearn.datasets import make_blobs\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot\n","from numpy import mean\n","from numpy import std\n","from numpy import array\n","from numpy import argmax\n","from numpy import tensordot\n","from numpy.linalg import norm\n","from itertools import product\n","\n","\n","\n","# fit model on dataset\n","\n","def fit_model(trainX, trainy):\n","\n","\ttrainy_enc = to_categorical(trainy)\n"," \n","\t# define model\n","\n","\tmodel = Sequential()\n"," \n","\tmodel.add(Dense(25, input_dim=2, activation='relu'))\n"," \n","\tmodel.add(Dense(3, activation='softmax'))\n"," \n","\tmodel.compile(loss='categorical_crossentropy', \n","               \n","               optimizer='adam', metrics=['accuracy'])\n"," \n","\t# fit model\n","\n","\tmodel.fit(trainX, trainy_enc, epochs=500, verbose=0)\n"," \n","\treturn model\n","\n","\n","\n","\n","# make an ensemble prediction for multi-class classification\n","\n","def ensemble_predictions(members, weights, testX):\n","\n","\t# make predictions\n","    \n","\n","\tyhats = [model.predict(testX) for model in members]\n","\n","\tyhats = array(yhats)\n"," \n","\t# weighted sum across ensemble members\n","\n","\tsummed = tensordot(yhats, weights, axes=((0),(0)))\n"," \n","\t# argmax across classes\n","\n","\tresult = argmax(summed, axis=1)\n"," \n","\treturn result\n","\n","\n","\n","\n","# evaluate a specific number of members in an ensemble\n","\n","def evaluate_ensemble(members, weights, testX, testy):\n","\n","\t# make prediction\n","\n","\tyhat = ensemble_predictions(members, weights, testX)\n"," \n","\t# calculate accuracy\n","\n","\treturn accuracy_score(testy, yhat)\n"," \n","\n","\n","# normalize a vector to have unit norm\n","\n","\n","def normalize(weights):\n","\n","\t# calculate l1 vector norm\n","\n","\tresult = norm(weights, 1)\n"," \n","\t# check for a vector of all zeros\n","\n","\tif result == 0.0:\n","\n","\t\treturn weights\n","\n","\t# return normalized vector (unit norm)\n","\n","\treturn weights / result\n","\n","\n","\n","\n","# grid search weights\n","\n","def grid_search(members, testX, testy):\n","\n","\t# define weights to consider\n","\n","\tw = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n","\n","\tbest_score, best_weights = 0.0, None\n","\n","\t# iterate all possible combinations (cartesian product)\n","\n","\tfor weights in product(w, repeat=len(members)):\n","     \n","\t\t# skip if all weights are equal\n","\n","\t\tif len(set(weights)) == 1:\n","\n","\t\t\tcontinue\n","\n","\t\t# hack, normalize weight vector\n","\n","\t\tweights = normalize(weights)\n","  \n","\t\t# evaluate weights\n","\n","\t\tscore = evaluate_ensemble(members, weights, testX, testy)\n","  \n","\t\tif score > best_score:\n","\n","\t\t\tbest_score, best_weights = score, weights\n","\n","\t\t\tprint('>%s %.3f' % (best_weights, best_score))\n","   \n","\treturn list(best_weights)\n"," \n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","print(trainX.shape, testX.shape)\n","\n","\n","\n","\n","# fit all models\n","\n","n_members = 5\n","\n","members = [fit_model(trainX, trainy) for _ in range(n_members)]\n","\n","\n","\n","\n","# evaluate each single model on the test set\n","\n","testy_enc = to_categorical(testy)\n","\n","for i in range(n_members):\n","\n","\t_, test_acc = members[i].evaluate(testX, testy_enc, verbose=0)\n"," \n","\tprint('Model %d: %.3f' % (i+1, test_acc))\n"," \n","\n","\n","# evaluate averaging ensemble (equal weights)\n","\n","weights = [1.0/n_members for _ in range(n_members)]\n","\n","score = evaluate_ensemble(members, weights, testX, testy)\n","\n","print('Equal Weights Score: %.3f' % score)\n","\n","\n","\n","\n","# grid search weights\n","\n","weights = grid_search(members, testX, testy)\n","\n","score = evaluate_ensemble(members, weights, testX, testy)\n","\n","print('Grid Search Weights: %s, Score: %.3f' % (weights, score))\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NGBnvZ4mt2Lt"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","21.2.5\n","\n","Weighted Average MLP Ensemble\n","\n","--\n","\n","Global Optimization to find coefficients for weighted ensemble on blobs problem.\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"jPe07DKAt2sU"},"source":["# global optimization to find coefficients \n","# for weighted ensemble on blobs problem\n","\n","from sklearn.datasets import make_blobs\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot\n","from numpy import mean\n","from numpy import std\n","from numpy import array\n","from numpy import argmax\n","from numpy import tensordot\n","from numpy.linalg import norm\n","from scipy.optimize import differential_evolution\n","\n","\n","\n","\n","# fit model on dataset\n","\n","def fit_model(trainX, trainy):\n","\n","\ttrainy_enc = to_categorical(trainy)\n"," \n","\t# define model\n","\n","\tmodel = Sequential()\n","\tmodel.add(Dense(25, input_dim=2, activation='relu'))\n","\tmodel.add(Dense(3, activation='softmax'))\n"," \n","\tmodel.compile(loss='categorical_crossentropy', \n","               \n","               optimizer='adam', metrics=['accuracy'])\n"," \n","\t# fit model\n","\n","\tmodel.fit(trainX, trainy_enc, epochs=500, verbose=0)\n"," \n","\treturn model\n","\n","\n","\n","\n","# make an ensemble prediction for multi-class classification\n","\n","def ensemble_predictions(members, weights, testX):\n","\n","\t# make predictions\n","\n","\tyhats = [model.predict(testX) for model in members]\n","\n","\tyhats = array(yhats)\n"," \n","\t# weighted sum across ensemble members\n","\n","\tsummed = tensordot(yhats, weights, axes=((0),(0)))\n"," \n","\t# argmax across classes\n","\n","\tresult = argmax(summed, axis=1)\n"," \n","\treturn result\n","\n","\n","\n","# # evaluate a specific number of members in an ensemble\n","\n","def evaluate_ensemble(members, weights, testX, testy):\n","\n","\t# make prediction\n","\n","\tyhat = ensemble_predictions(members, weights, testX)\n"," \n","\t# calculate accuracy\n","\n","\treturn accuracy_score(testy, yhat)\n"," \n","\n","\n","\n","# normalize a vector to have unit norm\n","\n","def normalize(weights):\n","\n","\t# calculate l1 vector norm\n","\n","\tresult = norm(weights, 1)\n"," \n","\t# check for a vector of all zeros\n","\n","\tif result == 0.0:\n","\n","\t\treturn weights\n","\n","\t# return normalized vector (unit norm)\n","\n","\treturn weights / result\n","\n","\n","\n","\n","# loss function for optimization process, designed to be minimized\n","\n","def loss_function(weights, members, testX, testy):\n","\n","\t# normalize weights\n","\n","\tnormalized = normalize(weights)\n"," \n","\t# calculate error rate\n","\n","\treturn 1.0 - evaluate_ensemble(members, normalized, testX, testy)\n"," \n","\n","\n","\n","# generate 2d classification dataset\n","\n","\n","X, y = make_blobs(n_samples=1100, \n","                  \n","                  centers=3, n_features=2, \n","                  \n","                  cluster_std=2, random_state=2)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","print(trainX.shape, testX.shape)\n","\n","\n","\n","# fit all models\n","\n","n_members = 5\n","\n","members = [fit_model(trainX, trainy) for _ in range(n_members)]\n","\n","\n","\n","\n","# evaluate each single model on the test set\n","\n","testy_enc = to_categorical(testy)\n","\n","\n","for i in range(n_members):\n","\n","\t_, test_acc = members[i].evaluate(testX, testy_enc, verbose=0)\n"," \n","\tprint('Model %d: %.3f' % (i+1, test_acc))\n"," \n","\n","\n","# evaluate averaging ensemble (equal weights)\n","\n","weights = [1.0/n_members for _ in range(n_members)]\n","\n","score = evaluate_ensemble(members, weights, testX, testy)\n","\n","print('Equal Weights Score: %.3f' % score)\n","\n","\n","\n","\n","# define bounds on each weight\n","\n","bound_w = [(0.0, 1.0)  for _ in range(n_members)]\n","\n","\n","\n","\n","# arguments to the loss function\n","\n","search_arg = (members, testX, testy)\n","\n","\n","\n","# global optimization of ensemble weights\n","\n","result = differential_evolution(loss_function, bound_w, \n","                                \n","                                search_arg, \n","                                \n","                                maxiter=1000, tol=1e-7)\n","\n","\n","\n","\n","# get the chosen weights\n","\n","weights = normalize(result['x'])\n","\n","print('Optimized Weights: %s' % weights)\n","\n","\n","\n","# evaluate chosen weights\n","\n","score = evaluate_ensemble(members, weights, testX, testy)\n","\n","print('Optimized Weights Score: %.3f' % score)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IIenM59QDKy8"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","22.2.2\n","\n","Single MLP Model\n","\n","--\n","\n","develop an MLP for blobs dataset\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"UOTOyfbyDLUQ"},"source":["# develop an mlp for blobs dataset\n","\n","from sklearn.datasets import make_blobs\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1000, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","\n","# split into train and test\n","\n","n_train = int(0.9 * X.shape[0])\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","model.add(Dense(50, input_dim=2, activation='relu'))\n","model.add(Dense(3, activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy', \n","              \n","              optimizer='adam', metrics=['accuracy'])\n","\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=50, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZRtTrCTOkLWx"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","22.2.3\n","\n","Random Splits Ensemble\n","\n","--\n","\n","Random Splits MLP Ensemble on blobs dataset\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"h6YPgL8mkLrm"},"source":["# random-splits mlp ensemble on blobs dataset\n","\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot\n","from numpy import mean\n","from numpy import std\n","import numpy\n","from numpy import array\n","from numpy import argmax\n","\n","\n","\n","\n","# evaluate a single mlp model\n","\n","def evaluate_model(trainX, trainy, testX, testy):\n","\n","\t# encode targets\n","\n","\ttrainy_enc = to_categorical(trainy)\n"," \n","\ttesty_enc = to_categorical(testy)\n"," \n","\t# define model\n","\n","\tmodel = Sequential()\n"," \n","\tmodel.add(Dense(50, input_dim=2, activation='relu'))\n"," \n","\tmodel.add(Dense(3, activation='softmax'))\n"," \n","\tmodel.compile(loss='categorical_crossentropy', \n","               \n","               optimizer='adam', metrics=['accuracy'])\n"," \n","\t# fit model\n","\n","\tmodel.fit(trainX, trainy_enc, epochs=50, verbose=0)\n"," \n","\t# evaluate the model\n","\n","\t_, test_acc = model.evaluate(testX, testy_enc, verbose=0)\n"," \n","\treturn model, test_acc\n","\n","\n","\n","\n","# make an ensemble prediction for multi-class classification\n","\n","def ensemble_predictions(members, testX):\n","\n","\t# make predictions\n","\n","\tyhats = [model.predict(testX) for model in members]\n","\n","\tyhats = array(yhats)\n"," \n","\t# sum across ensemble members\n","\n","\tsummed = numpy.sum(yhats, axis=0)\n"," \n","\t# argmax across classes\n","\n","\tresult = argmax(summed, axis=1)\n"," \n","\treturn result\n","\n","\n","\n","\n","# evaluate a specific number of members in an ensemble\n","\n","def evaluate_n_members(members, n_members, testX, testy):\n","\n","\t# select a subset of members\n","\n","\tsubset = members[:n_members]\n","\n","\t# make prediction\n","\n","\tyhat = ensemble_predictions(subset, testX)\n"," \n","\t# calculate accuracy\n","\n","\treturn accuracy_score(testy, yhat)\n"," \n","\n","\n","\n","# generate 2d classification dataset\n","\n","dataX, datay = make_blobs(n_samples=55000, \n","                          \n","                          centers=3, n_features=2, \n","                          \n","                          cluster_std=2, random_state=2)\n","\n","X, newX = dataX[:5000, :], dataX[5000:, :]\n","\n","y, newy = datay[:5000], datay[5000:]\n","\n","\n","\n","\n","# multiple train-test splits\n","\n","n_splits = 10\n","\n","scores, members = list(), list()\n","\n","for _ in range(n_splits):\n","\n","\t# split data\n","\n","\ttrainX, testX, trainy, testy = train_test_split(X, y, \n","                                                 \n","                                                 test_size=0.10)\n"," \n","\t# evaluate model\n","\n","\tmodel, test_acc = evaluate_model(trainX, \n","                                  \n","                                  trainy, testX, testy)\n"," \n","\tprint('>%.3f' % test_acc)\n"," \n","\tscores.append(test_acc)\n"," \n","\tmembers.append(model)\n"," \n","\n","\n","\n","# summarize expected performance\n","\n","print('Estimated Accuracy %.3f (%.3f)' % (mean(scores), std(scores)))\n","\n","\n","\n","\n","# evaluate different numbers of ensembles on hold out set\n","\n","single_scores, ensemble_scores = list(), list()\n","\n","for i in range(1, n_splits+1):\n","\n","\tensemble_score = evaluate_n_members(members, i, newX, newy)\n"," \n","\tnewy_enc = to_categorical(newy)\n"," \n","\t_, single_score = members[i-1].evaluate(newX, \n","                                         \n","                                         newy_enc, verbose=0)\n"," \n","\tprint('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n"," \n","\tensemble_scores.append(ensemble_score)\n"," \n","\tsingle_scores.append(single_score)\n"," \n","\n","\n","# plot score vs number of ensemble members\n","\n","print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))\n","\n","x_axis = [i for i in range(1, n_splits+1)]\n","\n","pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n","\n","pyplot.plot(x_axis, ensemble_scores, marker='o')\n","\n","pyplot.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bq8QzSE3rHsU"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","22.2.4\n","\n","Cross-Validation Ensemble\n","\n","--\n","\n","Cross-Validation MLP Ensemble on blobs dataset\n","\n","---\n","\n","--\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"YqrmTENUrIA6"},"source":["# cross-validation mlp ensemble on blobs dataset\n","\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot\n","from numpy import mean\n","from numpy import std\n","import numpy\n","from numpy import array\n","from numpy import argmax\n","\n","\n","\n","# evaluate a single mlp model\n","\n","def evaluate_model(trainX, trainy, testX, testy):\n","\n","\t# encode targets\n","\n","\ttrainy_enc = to_categorical(trainy)\n"," \n","\ttesty_enc = to_categorical(testy)\n"," \n","\t# define model\n","\n","\tmodel = Sequential()\n"," \n","\tmodel.add(Dense(50, input_dim=2, activation='relu'))\n"," \n","\tmodel.add(Dense(3, activation='softmax'))\n"," \n","\tmodel.compile(loss='categorical_crossentropy', \n","               \n","               optimizer='adam', metrics=['accuracy'])\n"," \n","\t# fit model\n","\n","\tmodel.fit(trainX, trainy_enc, epochs=50, verbose=0)\n"," \n","\t# evaluate the model\n","\n","\t_, test_acc = model.evaluate(testX, testy_enc, verbose=0)\n"," \n","\treturn model, test_acc\n","\n","\n","\n","\n","\n","# make an ensemble prediction for multi-class classification\n","\n","def ensemble_predictions(members, testX):\n","\n","\t# make predictions\n","\n","\tyhats = [model.predict(testX) for model in members]\n","\n","\tyhats = array(yhats)\n"," \n","\t# sum across ensemble members\n","\n","\tsummed = numpy.sum(yhats, axis=0)\n"," \n","\t# argmax across classes\n","\n","\tresult = argmax(summed, axis=1)\n"," \n","\treturn result\n","\n","\n","\n","\n","\n","# evaluate a specific number of members in an ensemble\n","\n","def evaluate_n_members(members, n_members, testX, testy):\n","\n","\t# select a subset of members\n","\n","\tsubset = members[:n_members]\n","\n","\t# make prediction\n","\n","\tyhat = ensemble_predictions(subset, testX)\n"," \n","\t# calculate accuracy\n","\n","\treturn accuracy_score(testy, yhat)\n"," \n","\n","\n","\n","# generate 2d classification dataset\n","\n","dataX, datay = make_blobs(n_samples=55000, \n","                          \n","                          centers=3, n_features=2, \n","                          \n","                          cluster_std=2, random_state=2)\n","\n","X, newX = dataX[:5000, :], dataX[5000:, :]\n","\n","y, newy = datay[:5000], datay[5000:]\n","\n","\n","\n","# prepare the k-fold cross-validation configuration\n","n_folds = 10\n","kfold = KFold(n_folds, True, 1)\n","\n","\n","\n","# cross validation estimation of performance\n","\n","scores, members = list(), list()\n","\n","for train_ix, test_ix in kfold.split(X):\n","\n","\t# select samples\n","\n","\ttrainX, trainy = X[train_ix], y[train_ix]\n","\n","\ttestX, testy = X[test_ix], y[test_ix]\n","\n","\t# evaluate model\n","\n","\tmodel, test_acc = evaluate_model(trainX, trainy, testX, testy)\n"," \n","\tprint('>%.3f' % test_acc)\n"," \n","\tscores.append(test_acc)\n"," \n","\tmembers.append(model)\n"," \n","# summarize expected performance\n","\n","print('Estimated Accuracy %.3f (%.3f)' % (mean(scores), std(scores)))\n","\n","# evaluate different numbers of ensembles on hold out set\n","\n","single_scores, ensemble_scores = list(), list()\n","\n","for i in range(1, n_folds+1):\n","\n","\tensemble_score = evaluate_n_members(members, i, newX, newy)\n"," \n","\tnewy_enc = to_categorical(newy)\n"," \n","\t_, single_score = members[i-1].evaluate(newX, \n","                                         \n","                                         newy_enc, verbose=0)\n"," \n","\tprint('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n"," \n","\tensemble_scores.append(ensemble_score)\n"," \n","\tsingle_scores.append(single_score)\n"," \n","\n","\n","\n","# plot score vs number of ensemble members\n","\n","print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))\n","\n","x_axis = [i for i in range(1, n_folds+1)]\n","\n","pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n","\n","pyplot.plot(x_axis, ensemble_scores, marker='o')\n","\n","pyplot.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YG3vDHXtuIMs"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","22.2.5\n","\n","Bagging Ensemble\n","\n","--\n","\n","Bagging MLP Ensemble on blobs dataset\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"exYmADhpuIge"},"source":["# bagging mlp ensemble on blobs dataset\n","from sklearn.datasets import make_blobs\n","from sklearn.utils import resample\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot\n","from numpy import mean\n","from numpy import std\n","import numpy\n","from numpy import array\n","from numpy import argmax\n","\n","\n","\n","\n","# evaluate a single mlp model\n","\n","def evaluate_model(trainX, trainy, testX, testy):\n","\n","\t# encode targets\n","\n","\ttrainy_enc = to_categorical(trainy)\n"," \n","\ttesty_enc = to_categorical(testy)\n"," \n","\t# define model\n","\n","\tmodel = Sequential()\n"," \n","\tmodel.add(Dense(50, input_dim=2, activation='relu'))\n"," \n","\tmodel.add(Dense(3, activation='softmax'))\n"," \n","\tmodel.compile(loss='categorical_crossentropy', \n","               \n","               optimizer='adam', metrics=['accuracy'])\n"," \n","\t# fit model\n","\n","\tmodel.fit(trainX, trainy_enc, epochs=50, verbose=0)\n"," \n","\t# evaluate the model\n","\n","\t_, test_acc = model.evaluate(testX, testy_enc, verbose=0)\n"," \n","\treturn model, test_acc\n","\n","\n","\n","\n","# make an ensemble prediction for multi-class classification\n","\n","def ensemble_predictions(members, testX):\n","\n","\t# make predictions\n","\n","\tyhats = [model.predict(testX) for model in members]\n","\n","\tyhats = array(yhats)\n"," \n","\t# sum across ensemble members\n","\n","\tsummed = numpy.sum(yhats, axis=0)\n"," \n","\t# argmax across classes\n","\n","\tresult = argmax(summed, axis=1)\n"," \n","\treturn result\n","\n","\n","\n","\n","# evaluate a specific number of members in an ensemble\n","\n","def evaluate_n_members(members, n_members, testX, testy):\n","\n","\t# select a subset of members\n","\n","\tsubset = members[:n_members]\n","\n","\t# make prediction\n","\n","\tyhat = ensemble_predictions(subset, testX)\n"," \n","\t# calculate accuracy\n","\n","\treturn accuracy_score(testy, yhat)\n"," \n","\n","\n","\n","# generate 2d classification dataset\n","\n","dataX, datay = make_blobs(n_samples=55000, \n","                          \n","                          centers=3, n_features=2, \n","                          \n","                          cluster_std=2, random_state=2)\n","\n","X, newX = dataX[:5000, :], dataX[5000:, :]\n","\n","y, newy = datay[:5000], datay[5000:]\n","\n","\n","\n","\n","# multiple train-test splits\n","\n","n_splits = 10\n","\n","scores, members = list(), list()\n","\n","for _ in range(n_splits):\n","\n","\t# select indexes\n","\n","\tix = [i for i in range(len(X))]\n","\n","\ttrain_ix = resample(ix, replace=True, n_samples=4500)\n"," \n","\ttest_ix = [x for x in ix if x not in train_ix]\n","\n","\t# select data\n","\n","\ttrainX, trainy = X[train_ix], y[train_ix]\n","\n","\ttestX, testy = X[test_ix], y[test_ix]\n","\n","\t# evaluate model\n","\n","\tmodel, test_acc = evaluate_model(trainX, trainy, testX, testy)\n"," \n","\tprint('>%.3f' % test_acc)\n"," \n","\tscores.append(test_acc)\n"," \n","\tmembers.append(model)\n"," \n","\n","\n","\n","# summarize expected performance\n","\n","print('Estimated Accuracy %.3f (%.3f)' % (mean(scores), std(scores)))\n","\n","\n","\n","\n","# evaluate different numbers of ensembles on hold out set\n","\n","single_scores, ensemble_scores = list(), list()\n","\n","for i in range(1, n_splits+1):\n","\n","\tensemble_score = evaluate_n_members(members, i, newX, newy)\n"," \n","\tnewy_enc = to_categorical(newy)\n"," \n","\t_, single_score = members[i-1].evaluate(newX, newy_enc, verbose=0)\n"," \n","\tprint('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n"," \n","\tensemble_scores.append(ensemble_score)\n"," \n","\tsingle_scores.append(single_score)\n"," \n","\n","\n","\n","# plot score vs number of ensemble members\n","\n","print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))\n","\n","x_axis = [i for i in range(1, n_splits+1)]\n","\n","pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n","\n","pyplot.plot(x_axis, ensemble_scores, marker='o')\n","\n","pyplot.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lEcWVVNK1W-o"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","23.2.2\n","\n","MLP Model\n","\n","--\n","\n","Develop an MLP model for blobs dataset\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"iSaCwL-n1XSD"},"source":["# develop an mlp for blobs dataset\n","\n","from sklearn.datasets import make_blobs\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","print(trainX.shape, testX.shape)\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(25, input_dim=2, activation='relu'))\n","\n","model.add(Dense(3, activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy',      \n","              \n","              optimizer='adam', metrics=['accuracy'])\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=1000, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WYuZLPlIO8W2"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","23.2.3\n","\n","Save Horizontal Models\n","\n","--\n","\n","save horizontal voting ensemble members during training\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"HkLWwYOcO8sT"},"source":["# save horizontal voting ensemble members during training\n","\n","from sklearn.datasets import make_blobs\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot\n","from os import makedirs\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","print(trainX.shape, testX.shape)\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(25, input_dim=2, activation='relu'))\n","\n","model.add(Dense(3, activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy', \n","              \n","              optimizer='adam', \n","              \n","              metrics=['accuracy'])\n","\n","\n","\n","\n","# create directory for models\n","\n","makedirs('models')\n","\n","\n","\n","\n","# fit model\n","\n","n_epochs, n_save_after = 1000, 950\n","\n","for i in range(n_epochs):\n","\n","\t# fit model for a single epoch\n","\n","\tmodel.fit(trainX, trainy, epochs=1, verbose=0)\n"," \n","\t# check if we should save the model\n","\n","\tif i >= n_save_after:\n","\n","\t\tmodel.save('models/model_' + str(i) + '.h5')\n","  \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DVcolv8OSvfJ"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","23.2.3\n","\n","Make Horizontal Ensemble Models\n","\n","--\n","\n","load models and make predictions using a horizontal voting ensemble\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"wixFCtaaSvx-"},"source":["# load models and make predictions using \n","# a horizontal voting ensemble\n","\n","from sklearn.datasets import make_blobs\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot\n","from numpy import mean\n","from numpy import std\n","import numpy\n","from numpy import array\n","from numpy import argmax\n","\n","\n","\n","# load models from file\n","\n","def load_all_models(n_start, n_end):\n","\n","\tall_models = list()\n"," \n","\tfor epoch in range(n_start, n_end):\n","     \n","\t\t# define filename for this ensemble\n","\n","\t\tfilename = 'models/model_' + str(epoch) + '.h5'\n","\n","\t\t# load model from file\n","\n","\t\tmodel = load_model(filename)\n","  \n","\t\t# add to list of members\n","\n","\t\tall_models.append(model)\n","  \n","\t\tprint('>loaded %s' % filename)\n","  \n","\treturn all_models\n","\n","\n","\n","\n","# make an ensemble prediction for multi-class classification\n","\n","def ensemble_predictions(members, testX):\n","\n","\t# make predictions\n","\n","\tyhats = [model.predict(testX) for model in members]\n","\n","\tyhats = array(yhats)\n"," \n","\t# sum across ensemble members\n","\n","\tsummed = numpy.sum(yhats, axis=0)\n"," \n","\t# argmax across classes\n","\n","\tresult = argmax(summed, axis=1)\n"," \n","\treturn result\n","\n","\n","\n","\n","# evaluate a specific number of members in an ensemble\n","\n","def evaluate_n_members(members, n_members, testX, testy):\n","\n","\t# select a subset of members\n","\n","\tsubset = members[:n_members]\n","\n","\t# make prediction\n","\n","\tyhat = ensemble_predictions(subset, testX)\n"," \n","\t# calculate accuracy\n","\n","\treturn accuracy_score(testy, yhat)\n"," \n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, \n","                  \n","                  centers=3, n_features=2, \n","                  \n","                  cluster_std=2, random_state=2)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","print(trainX.shape, testX.shape)\n","\n","\n","\n","\n","# load models in order\n","\n","members = load_all_models(950, 1000)\n","\n","print('Loaded %d models' % len(members))\n","\n","\n","\n","\n","# reverse loaded models so we build \n","# the ensemble with the last models first\n","\n","members = list(reversed(members))\n","\n","\n","\n","\n","# evaluate different numbers of ensembles on hold out set\n","\n","single_scores, ensemble_scores = list(), list()\n","\n","for i in range(1, len(members)+1):\n","\n","    # evaluate model with i members\n","\n","    ensemble_score = evaluate_n_members(members, \n","                                        \n","                                        i, testX, testy)\n","\n","    # evaluate the i'th model standalone\n","\n","    testy_enc = to_categorical(testy)\n","\n","    _, single_score = members[i-1].evaluate(testX,\n","                                            \n","                                            testy_enc, \n","\n","                                            verbose=0)\n","\n","    # summarize this step\n","\n","    print('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n","\n","    single_scores.append(single_score)\n","    ensemble_scores.append(ensemble_score)\n"," \n","\n","\n","\n","# summarize average accuracy of a single final model\n","\n","print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))\n","\n","\n","\n","\n","# plot score vs number of ensemble members\n","\n","x_axis = [i for i in range(1, len(members)+1)]\n","\n","pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n","\n","pyplot.plot(x_axis, ensemble_scores, marker='o')\n","\n","pyplot.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oyr4ccI0rtlL"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","24.2.2\n","\n","Multi-Layer Perceptron Model\n","\n","--\n","\n","develop an MLP for blobs dataset\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"50ej4jqDrt07"},"source":["# develop an mlp for blobs dataset\n","\n","from sklearn.datasets import make_blobs\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD\n","from matplotlib import pyplot as plt\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(25, input_dim=2, activation='relu'))\n","\n","model.add(Dense(3, activation='softmax'))\n","\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss='categorical_crossentropy', \n","              \n","              optimizer=opt, \n","              \n","              metrics=['accuracy'])\n","\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=200, verbose=0)\n","\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZASR-wiu12xV"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","24.2.3\n","\n","Cosine Annealing Learning Rate\n","\n","--\n","\n","MLP with Cosine Annealing Learning Rate Schedule on blobs dataset\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"N3MVg_wg13DI"},"source":["# mlp with cosine annealing learning \n","# rate schedule on blobs problem\n","\n","from sklearn.datasets import make_blobs\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.callbacks import Callback\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras import backend\n","from math import pi\n","from math import cos\n","from math import floor\n","from matplotlib import pyplot as plt\n","\n","\n","\n","\n","# define custom learning rate schedule\n","\n","class CosineAnnealingLearningRateSchedule(Callback):\n","\n","\n","    # constructor\n","    def __init__(self, n_epochs, n_cycles, lrate_max, verbose=0):\n","        self.epochs = n_epochs\n","        self.cycles = n_cycles\n","        self.lr_max = lrate_max\n","        self.lrates = list()\n","\n","\n","    # calculate learning rate for an epoch\n","    def cosine_annealing(self, epoch, n_epochs, n_cycles, lrate_max):\n","        epochs_per_cycle = floor(n_epochs/n_cycles)\n","        cos_inner = (pi * (epoch % epochs_per_cycle)) / (epochs_per_cycle)\n","        return lrate_max/2 * (cos(cos_inner) + 1)\n","\n","\n","    # calculate and set learning rate at the start of the epoch\n","    def on_epoch_begin(self, epoch, logs=None):\n","        # calculate learning rate\n","        lr = self.cosine_annealing(epoch, self.epochs, self.cycles, self.lr_max)\n","        # set learning rate\n","        backend.set_value(self.model.optimizer.lr, lr)\n","        # log value\n","        self.lrates.append(lr)\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(25, input_dim=2, activation='relu'))\n","\n","model.add(Dense(3, activation='softmax'))\n","\n","opt = SGD(momentum=0.9)\n","\n","model.compile(loss='categorical_crossentropy', \n","              \n","              optimizer=opt, \n","              \n","              metrics=['accuracy'])\n","\n","\n","# define learning rate callback\n","\n","n_epochs = 400\n","\n","n_cycles = n_epochs / 50\n","\n","ca = CosineAnnealingLearningRateSchedule(n_epochs, \n","                                         \n","                                         n_cycles, \n","                                         \n","                                         0.01)\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=n_epochs, verbose=0, \n","                    \n","                    callbacks=[ca])\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot learning rate\n","\n","plt.plot(ca.lrates)\n","\n","plt.show()\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WoMbMH7L6wlB"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","24.2.4\n","\n","MLP Snapshot Ensemble\n","\n","--\n","\n","A.\n","\n","Save Snapshot Models During Training\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"NWIrtRD86w23"},"source":["# example of saving models for a snapshot ensemble\n","\n","from sklearn.datasets import make_blobs\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.callbacks import Callback\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras import backend\n","from math import pi\n","from math import cos\n","from math import floor\n","\n","\n","\n","# snapshot ensemble with custom learning rate schedule\n","\n","class SnapshotEnsemble(Callback):\n","\n","\t# constructor\n","\n","\tdef __init__(self, n_epochs, n_cycles, lrate_max, verbose=0):\n","\t\tself.epochs = n_epochs\n","\t\tself.cycles = n_cycles\n","\t\tself.lr_max = lrate_max\n","\t\tself.lrates = list()\n","\n","\t# calculate learning rate for epoch\n","\n","\tdef cosine_annealing(self, epoch, n_epochs, n_cycles, lrate_max):\n","\t\tepochs_per_cycle = floor(n_epochs/n_cycles)\n","\t\tcos_inner = (pi * (epoch % epochs_per_cycle)) / (epochs_per_cycle)\n","\t\treturn lrate_max/2 * (cos(cos_inner) + 1)\n","\n","\t# calculate and set learning rate at the start of the epoch\n","\n","\tdef on_epoch_begin(self, epoch, logs={}):\n","     \n","\t\t# calculate learning rate\n","\n","\t\tlr = self.cosine_annealing(epoch, \n","                             \n","                             self.epochs, \n","                             \n","                             self.cycles, \n","                             \n","                             self.lr_max)\n","  \n","\t\t# set learning rate\n","\n","\t\tbackend.set_value(self.model.optimizer.lr, lr)\n","  \n","\t\t# log value\n","\n","\t\tself.lrates.append(lr)\n","\n","\t# save models at the end of each cycle\n","\n","\tdef on_epoch_end(self, epoch, logs={}):\n","     \n","\t\t# check if we can save model\n","\n","\t\tepochs_per_cycle = floor(self.epochs / self.cycles)\n","  \n","\t\tif epoch != 0 and (epoch + 1) % epochs_per_cycle == 0:\n","\n","\t\t\t# save model to file\n","\n","\t\t\tfilename = \"snapshot_model_%d.h5\" % int((epoch + 1) / epochs_per_cycle)\n","   \n","\t\t\tself.model.save(filename)\n","   \n","\t\t\tprint('>saved snapshot %s, epoch %d' % (filename, epoch))\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, \n","                  \n","                  centers=3, n_features=2, \n","                  \n","                  cluster_std=2, random_state=2)\n","\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(50, input_dim=2, activation='relu'))\n","\n","model.add(Dense(3, activation='softmax'))\n","\n","opt = SGD(momentum=0.9)\n","\n","model.compile(loss='categorical_crossentropy', \n","              \n","              optimizer=opt, \n","              \n","              metrics=['accuracy'])\n","\n","\n","\n","# create snapshot ensemble callback\n","\n","\n","n_epochs = 500\n","\n","n_cycles = n_epochs / 50\n","\n","ca = SnapshotEnsemble(n_epochs, n_cycles, 0.01)\n","\n","# fit model\n","\n","model.fit(trainX, trainy, \n","          \n","          validation_data=(testX, testy), \n","          \n","          epochs=n_epochs, \n","          \n","          verbose=0, \n","          \n","          callbacks=[ca])\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VGLo6oJf7_Xx"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","24.2.4\n","\n","MLP Snapshot Ensemble\n","\n","--\n","\n","B.\n","\n","Load Models and Make a Snapshot Ensemble Prediction\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"FQ9ATr-k7_td"},"source":["# load models and make a snapshot ensemble prediction\n","\n","from sklearn.datasets import make_blobs\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot\n","from numpy import mean\n","from numpy import std\n","import numpy\n","from numpy import array\n","from numpy import argmax\n","\n","# load models from file\n","\n","def load_all_models(n_models):\n","\n","\tall_models = list()\n"," \n","\tfor i in range(n_models):\n","     \n","\t\t# define filename for this ensemble\n","\n","\t\tfilename = 'snapshot_model_' + str(i + 1) + '.h5'\n","\n","\t\t# load model from file\n","\n","\t\tmodel = load_model(filename)\n","  \n","\t\t# add to list of members\n","\n","\t\tall_models.append(model)\n","  \n","\t\tprint('>loaded %s' % filename)\n","  \n","\treturn all_models\n","\n","# make an ensemble prediction for multi-class classification\n","\n","def ensemble_predictions(members, testX):\n","\n","\t# make predictions\n","\n","\tyhats = [model.predict(testX) for model in members]\n","\n","\tyhats = array(yhats)\n"," \n","\t# sum across ensemble members\n","\n","\tsummed = numpy.sum(yhats, axis=0)\n"," \n","\t# argmax across classes\n","\n","\tresult = argmax(summed, axis=1)\n"," \n","\treturn result\n","\n","\n","\n","\n","# evaluate a specific number of members in an ensemble\n","\n","def evaluate_n_members(members, n_members, testX, testy):\n","\n","\t# select a subset of members\n","\n","\tsubset = members[:n_members]\n","\n","\t# make prediction\n","\n","\tyhat = ensemble_predictions(subset, testX)\n"," \n","\t# calculate accuracy\n","\n","\treturn accuracy_score(testy, yhat)\n"," \n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, \n","                  \n","                  centers=3, \n","                  \n","                  n_features=2, \n","                  \n","                  cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","print(trainX.shape, testX.shape)\n","\n","\n","\n","# load models in order\n","\n","members = load_all_models(10)\n","\n","print('Loaded %d models' % len(members))\n","\n","\n","\n","\n","# reverse loaded models so we build the \n","# ensemble with the last models first\n","\n","members = list(reversed(members))\n","\n","\n","\n","# evaluate different numbers of ensembles on hold out set\n","\n","single_scores, ensemble_scores = list(), list()\n","\n","for i in range(1, len(members)+1):\n","\n","\t# evaluate model with i members\n","\n","\tensemble_score = evaluate_n_members(members, i, testX, testy)\n"," \n","\t# evaluate the i'th model standalone\n","\n","\ttesty_enc = to_categorical(testy)\n"," \n","\t_, single_score = members[i-1].evaluate(testX, \n","                                         \n","                                         testy_enc, \n","                                         \n","                                         verbose=0)\n"," \n","\t# summarize this step\n","\n","\tprint('> %d: single=%.3f, ensemble=%.3f' % (i, \n","                                             \n","                                             single_score, \n","                                             \n","                                             ensemble_score))\n"," \n","\tensemble_scores.append(ensemble_score)\n"," \n","\tsingle_scores.append(single_score)\n"," \n","\n","\n","\n","# summarize average accuracy of a single final model\n","\n","print('Accuracy %.3f (%.3f)' % (mean(single_scores), \n","                                \n","                                std(single_scores)))\n","\n","\n","\n","# plot score vs number of ensemble members\n","\n","x_axis = [i for i in range(1, len(members)+1)]\n","\n","pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n","\n","pyplot.plot(x_axis, ensemble_scores, marker='o')\n","\n","pyplot.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t1l4o5Pa8XEQ"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","25.2.2\n","\n","MLP Model\n","\n","--\n","\n","develop an MLP for blobs dataset\n","\n","https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"SOMxltlL8XhV"},"source":["# develop an mlp for blobs dataset\n","\n","from sklearn.datasets import make_blobs\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot as plt\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","print(trainX.shape, testX.shape)\n","\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(25, \n","                \n","                input_dim=2, activation='relu'))\n","\n","model.add(Dense(3, \n","                \n","                activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy', \n","              \n","              optimizer='adam', \n","              \n","              metrics=['accuracy'])\n","\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=500, verbose=0)\n","\n","\n","\n","# evaluate the model\n","_, train_acc = model.evaluate(trainX, \n","                              \n","                              trainy, verbose=0)\n","\n","_, test_acc = model.evaluate(testX, \n","                             \n","                             testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uk8xEjti0J-W"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","25.2.3 \n","\n","Train and Save Sub-Models\n","\n","--\n","\n","example of saving sub-models for later use in a stacking ensemble\n","\n","https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"Xg2yd4aD0KUN"},"source":["# example of saving sub-models for \n","# later use in a stacking ensemble\n","\n","from sklearn.datasets import make_blobs\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot\n","from os import makedirs\n","\n","\n","\n","# fit model on dataset\n","\n","def fit_model(trainX, trainy):\n","\t# define model\n","\tmodel = Sequential()\n","\tmodel.add(Dense(25, input_dim=2, activation='relu'))\n","\tmodel.add(Dense(3, activation='softmax'))\n"," \n","\tmodel.compile(loss='categorical_crossentropy', \n","               \n","               optimizer='adam', \n","               \n","               metrics=['accuracy'])\n","\t# fit model\n","\tmodel.fit(trainX, trainy, epochs=500, verbose=0)\n","\treturn model\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, \n","                  \n","                  centers=3, n_features=2, \n","                  \n","                  cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","print(trainX.shape, testX.shape)\n","\n","\n","\n","# create directory for models\n","\n","makedirs('blend_models')\n","\n","\n","\n","\n","# fit and save models\n","\n","n_members = 5\n","\n","for i in range(n_members):\n","\n","\t# fit model\n","\n","\tmodel = fit_model(trainX, trainy)\n"," \n","\t# save model\n","\n","\tfilename = 'blend_models/model_' + str(i + 1) + '.h5'\n","\n","\tmodel.save(filename)\n"," \n","\tprint('>Saved %s' % filename)\n"," \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yW6Av2Rj2q04"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","25.2.4 \n","\n","Separate Stacking Model\n","\n","--\n","\n","stacked generalization with linear meta model on blobs dataset\n","\n","https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"0AgFMlHG2rGU"},"source":["# stacked generalization with linear meta \n","# model on blobs dataset\n","\n","from sklearn.datasets import make_blobs\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.utils import to_categorical\n","from numpy import dstack\n","\n","\n","\n","# load models from file\n","\n","def load_all_models(n_models):\n","\n","\tall_models = list()\n"," \n","\tfor i in range(n_models):\n","     \n","\t\t# define filename for this ensemble\n","\n","\t\tfilename = 'blend_models/model_' + str(i + 1) + '.h5'\n","\n","\t\t# load model from file\n","\n","\t\tmodel = load_model(filename)\n","  \n","\t\t# add to list of members\n","\n","\t\tall_models.append(model)\n","  \n","\t\tprint('>loaded %s' % filename)\n","  \n","\treturn all_models\n","\n","\n","\n","\n","# create stacked model input dataset \n","# as outputs from the ensemble\n","\n","def stacked_dataset(members, inputX):\n","\n","\tstackX = None\n","\n","\tfor model in members:\n","\n","\t\t# make prediction\n","\n","\t\tyhat = model.predict(inputX, verbose=0)\n","  \n","\t\t# stack predictions into [rows, members, probabilities]\n","\n","\t\tif stackX is None:\n","\n","\t\t\tstackX = yhat\n","\n","\t\telse:\n","\n","\t\t\tstackX = dstack((stackX, yhat))\n","   \n","\t# flatten predictions to [rows, members x probabilities]\n","\n","\tstackX = stackX.reshape((stackX.shape[0], \n","                          \n","                          stackX.shape[1]*stackX.shape[2]))\n"," \n","\treturn stackX\n","\n","\n","\n","# fit a model based on the outputs from the ensemble members\n","\n","def fit_stacked_model(members, inputX, inputy):\n","\n","\t# create dataset using ensemble\n","\n","\tstackedX = stacked_dataset(members, inputX)\n"," \n","\t# fit standalone model\n","\n","\tmodel = LogisticRegression()\n"," \n","\tmodel.fit(stackedX, inputy)\n"," \n","\treturn model\n","\n","\n","\n","\n","# make a prediction with the stacked model\n","\n","def stacked_prediction(members, model, inputX):\n","\n","\t# create dataset using ensemble\n","\n","\tstackedX = stacked_dataset(members, inputX)\n"," \n","\t# make a prediction\n","\n","\tyhat = model.predict(stackedX)\n"," \n","\treturn yhat\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, \n","                  \n","                  centers=3, n_features=2, \n","                  \n","                  cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","print(trainX.shape, testX.shape)\n","\n","\n","\n","\n","# load all models\n","\n","n_members = 5\n","\n","members = load_all_models(n_members)\n","\n","print('Loaded %d models' % len(members))\n","\n","\n","\n","\n","\n","# evaluate standalone models on test dataset\n","\n","for model in members:\n","\n","\ttesty_enc = to_categorical(testy)\n"," \n","\t_, acc = model.evaluate(testX, testy_enc, verbose=0)\n"," \n","\tprint('Model Accuracy: %.3f' % acc)\n"," \n","\n","\n","\n","\n","# fit stacked model using the ensemble\n","\n","model = fit_stacked_model(members, testX, testy)\n","\n","\n","\n","\n","# evaluate model on test set\n","\n","yhat = stacked_prediction(members, model, testX)\n","\n","acc = accuracy_score(testy, yhat)\n","\n","print('Stacked Test Accuracy: %.3f' % acc)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rEqnfyer6tq2"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","25.2.5 \n","\n","Integrated Stacking Model\n","\n","--\n","\n","stacked generalization with neural net meta model on blobs dataset\n","\n","https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"fv2GjH-z6t7u"},"source":["# stacked generalization with neural \n","# net meta model on blobs dataset\n","\n","from sklearn.datasets import make_blobs\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Dense\n","from keras.layers.merge import concatenate\n","from numpy import argmax\n","\n","\n","\n","# load models from file\n","\n","def load_all_models(n_models):\n","\tall_models = list()\n","\tfor i in range(n_models):\n","\t\t# define filename for this ensemble\n","\t\tfilename = 'blend_models/model_' + str(i + 1) + '.h5'\n","\t\t# load model from file\n","\t\tmodel = load_model(filename)\n","\t\t# add to list of members\n","\t\tall_models.append(model)\n","\t\tprint('>loaded %s' % filename)\n","\treturn all_models\n","\n","\n","\n","\n","# define stacked model from multiple member input models\n","\n","def define_stacked_model(members):\n","\t# update all layers in all models to not be trainable\n","\tfor i in range(len(members)):\n","\t\tmodel = members[i]\n","\t\tfor layer in model.layers:\n","\t\t\t# make not trainable\n","\t\t\tlayer.trainable = False\n","\t\t\t# rename to avoid 'unique layer name' issue\n","\t\t\tlayer._name = 'ensemble_' + str(i+1) + '_' + layer.name\n","\n","\t# define multi-headed input\n","\tensemble_visible = [model.input for model in members]\n","\n","\t# concatenate merge output from each model\n","\tensemble_outputs = [model.output for model in members]\n","\tmerge = concatenate(ensemble_outputs)\n","\thidden = Dense(10, activation='relu')(merge)\n","\toutput = Dense(3, activation='softmax')(hidden)\n","\tmodel = Model(inputs=ensemble_visible, outputs=output)\n"," \n","\t# plot graph of ensemble\n","\tplot_model(model, show_shapes=True, to_file='model_graph.png')\n"," \n","\t# compile\n","\tmodel.compile(loss='categorical_crossentropy', \n","               \n","               optimizer='adam', \n","               \n","               metrics=['accuracy'])\n"," \n","\treturn model\n","\n","\n","\n","\n","# fit a stacked model\n","\n","def fit_stacked_model(model, inputX, inputy):\n","\t# prepare input data\n","\tX = [inputX for _ in range(len(model.input))]\n","\t# encode output data\n","\tinputy_enc = to_categorical(inputy)\n","\t# fit model\n","\tmodel.fit(X, inputy_enc, epochs=300, verbose=0)\n"," \n","\n","\n","\n","# make a prediction with a stacked model\n","\n","def predict_stacked_model(model, inputX):\n","\t# prepare input data\n","\tX = [inputX for _ in range(len(model.input))]\n","\t# make prediction\n","\treturn model.predict(X, verbose=0)\n"," \n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","print(trainX.shape, testX.shape)\n","\n","\n","\n","\n","# load all models\n","\n","n_members = 5\n","\n","members = load_all_models(n_members)\n","\n","print('Loaded %d models' % len(members))\n","\n","\n","\n","\n","# define ensemble model\n","\n","stacked_model = define_stacked_model(members)\n","\n","\n","\n","\n","# fit stacked model on test dataset\n","\n","fit_stacked_model(stacked_model, testX, testy)\n","\n","\n","\n","\n","# make predictions and evaluate\n","\n","yhat = predict_stacked_model(stacked_model, testX)\n","\n","yhat = argmax(yhat, axis=1)\n","\n","acc = accuracy_score(testy, yhat)\n","\n","print('Stacked Test Accuracy: %.3f' % acc)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JQcUndyf_Ntt"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","26.2.2 \n","\n","Multi-Layer Perceptron Model\n","\n","--\n","\n","develop an MLP for blobs dataset\n","\n","https://machinelearningmastery.com/polyak-neural-network-model-weight-ensemble/\n","\n","\n","Also look at:\n","\n","https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"n186FfrP_OCV"},"source":["# develop an mlp for blobs dataset\n","\n","from sklearn.datasets import make_blobs\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","# split into train and test\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","print(trainX.shape, testX.shape)\n","\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(25, input_dim=2, activation='relu'))\n","\n","model.add(Dense(3, activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy', \n","              \n","              optimizer='adam', \n","              \n","              metrics=['accuracy'])\n","\n","\n","\n","\n","# fit model\n","\n","history = model.fit(trainX, trainy, \n","                    \n","                    validation_data=(testX, testy), \n","                    \n","                    epochs=500, verbose=0)\n","\n","\n","\n","# evaluate the model\n","\n","_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n","_, test_acc = model.evaluate(testX, testy, verbose=0)\n","\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","\n","\n","\n","# plot loss learning curves\n","\n","plt.subplot(211)\n","\n","plt.title('Cross-Entropy Loss', pad=-40)\n","\n","plt.plot(history.history['loss'], label='train')\n","\n","plt.plot(history.history['val_loss'], label='test')\n","\n","plt.legend()\n","\n","# plt.show()\n","\n","\n","\n","# plot accuracy learning curves\n","\n","plt.subplot(212)\n","\n","plt.title('Accuracy', pad=-40)\n","\n","plt.plot(history.history['accuracy'], label='train')\n","\n","plt.plot(history.history['val_accuracy'], label='test')\n","\n","\n","\n","plt.legend()\n","\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O9bAu-5mJFB_"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","26.2.3 \n","\n","Save Multiple Models to File\n","\n","--\n","\n","Save Models to File towards the end of a training sum\n","\n","https://machinelearningmastery.com/polyak-neural-network-model-weight-ensemble/\n","\n","Also look at:\n","\n","https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"wOy0c9AEJFUS"},"source":["# save models to file toward the end of a training run\n","\n","from sklearn.datasets import make_blobs\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","\n","# define model\n","\n","model = Sequential()\n","\n","model.add(Dense(25, input_dim=2, activation='relu'))\n","\n","model.add(Dense(3, activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy', \n","              \n","              optimizer='adam', \n","              \n","              metrics=['accuracy'])\n","\n","\n","\n","\n","# fit model\n","\n","n_epochs, n_save_after = 500, 490\n","\n","for i in range(n_epochs):\n","\n","\t# fit model for a single epoch\n","\n","\tmodel.fit(trainX, trainy, epochs=1, verbose=0)\n"," \n","\t# check if we should save the model\n","\n","\tif i >= n_save_after:\n","\n","\t\tmodel.save('model_' + str(i) + '.h5')\n","  \n","\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BxEQZkn5NICC"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","26.2.4 \n","\n","New Model with Average Model Weights\n","\n","--\n","\n","Average the weights of multiple loaded models\n","\n","https://machinelearningmastery.com/polyak-neural-network-model-weight-ensemble/\n","\n","Also look at:\n","\n","https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"qmS08pi6NITA"},"source":["# average the weights of multiple loaded models\n","\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.models import clone_model\n","from numpy import average\n","from numpy import array\n","\n","\n","\n","\n","# load models from file\n","\n","def load_all_models(n_start, n_end):\n","\n","\tall_models = list()\n"," \n","\tfor epoch in range(n_start, n_end):\n","     \n","\t\t# define filename for this ensemble\n","\n","\t\tfilename = 'model_' + str(epoch) + '.h5'\n","\n","\t\t# load model from file\n","\n","\t\tmodel = load_model(filename)\n","  \n","\t\t# add to list of members\n","\n","\t\tall_models.append(model)\n","  \n","\t\tprint('>loaded %s' % filename)\n","  \n","\treturn all_models\n","\n","\n","\n","\n","# create a model from the weights of multiple models\n","\n","def model_weight_ensemble(members, weights):\n","\n","\t# determine how many layers need to be averaged\n","\n","\tn_layers = len(members[0].get_weights())\n"," \n","\t# create an set of average model weights\n","\n","\tavg_model_weights = list()\n"," \n","\tfor layer in range(n_layers):\n","     \n","\t\t# collect this layer from each model\n","\n","\t\tlayer_weights = array([model.get_weights()[layer] for model in members])\n","  \n","\t\t# weighted average of weights for this layer\n","\n","\t\tavg_layer_weights = average(layer_weights, \n","                              \n","                              axis=0, weights=weights)\n","  \n","\t\t# store average layer weights\n","\n","\t\tavg_model_weights.append(avg_layer_weights)\n","  \n","\t# create a new model with the same structure\n","\n","\tmodel = clone_model(members[0])\n"," \n","\t# set the weights in the new\n","\n","\tmodel.set_weights(avg_model_weights)\n"," \n","\tmodel.compile(loss='categorical_crossentropy', \n","               \n","               optimizer='adam', metrics=['accuracy'])\n"," \n","\treturn model\n","\n","\n","\n","\n","\n","# load all models into memory\n","\n","members = load_all_models(490, 500)\n","\n","print('Loaded %d models' % len(members))\n","\n","\n","\n","\n","\n","# prepare an array of equal weights\n","\n","n_models = len(members)\n","\n","weights = [1/n_models for i in range(1, n_models+1)]\n","\n","\n","\n","\n","\n","# create a new model with the weighted average of all model weights\n","\n","\n","\n","model = model_weight_ensemble(members, weights)\n","\n","\n","# summarize the created model\n","\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xmfaY9VfPxrh"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","26.2.5 \n","\n","Predicting with an average model weight ensemble\n","\n","--\n","\n","Average of model weights on blobs problem\n","\n","https://machinelearningmastery.com/polyak-neural-network-model-weight-ensemble/\n","\n","Also look at:\n","\n","https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"BW37ysffPx1o"},"source":["# average of model weights on blobs problem\n","\n","from sklearn.datasets import make_blobs\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.models import clone_model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot\n","from numpy import average\n","from numpy import array\n","\n","\n","\n","\n","# load models from file\n","\n","def load_all_models(n_start, n_end):\n","\n","\tall_models = list()\n"," \n","\tfor epoch in range(n_start, n_end):\n","     \n","\t\t# define filename for this ensemble\n","\n","\t\tfilename = 'model_' + str(epoch) + '.h5'\n","\n","\t\t# load model from file\n","\n","\t\tmodel = load_model(filename)\n","  \n","\t\t# add to list of members\n","\n","\t\tall_models.append(model)\n","  \n","\t\tprint('>loaded %s' % filename)\n","  \n","\treturn all_models\n","\n","\n","\n","\n","\n","# # create a model from the weights of multiple models\n","\n","def model_weight_ensemble(members, weights):\n","\n","\t# determine how many layers need to be averaged\n","\n","\tn_layers = len(members[0].get_weights())\n"," \n","\t# create an set of average model weights\n","\n","\tavg_model_weights = list()\n"," \n","\tfor layer in range(n_layers):\n","     \n","\t\t# collect this layer from each model\n","\n","\t\tlayer_weights = array([model.get_weights()[layer] for model in members])\n","  \n","\t\t# weighted average of weights for this layer\n","\n","\t\tavg_layer_weights = average(layer_weights, \n","                              \n","                              axis=0, weights=weights)\n","  \n","\t\t# store average layer weights\n","\n","\t\tavg_model_weights.append(avg_layer_weights)\n","  \n","\t# create a new model with the same structure\n","\n","\tmodel = clone_model(members[0])\n"," \n","\t# set the weights in the new\n","\n","\tmodel.set_weights(avg_model_weights)\n"," \n","\tmodel.compile(loss='categorical_crossentropy', \n","               \n","               optimizer='adam', \n","               \n","               metrics=['accuracy'])\n"," \n","\treturn model\n","\n","\n","\n","\n","# evaluate a specific number of members in an ensemble\n","\n","def evaluate_n_members(members, n_members, testX, testy):\n","\n","\t# select a subset of members\n","\n","\tsubset = members[:n_members]\n","\n","\t# prepare an array of equal weights\n","\n","\tweights = [1.0/n_members for i in range(1, n_members+1)]\n","\n","\t# create a new model with the weighted average of all model weights\n","\n","\tmodel = model_weight_ensemble(subset, weights)\n"," \n","\t# make predictions and evaluate accuracy\n","\n","\t_, test_acc = model.evaluate(testX, \n","                              \n","                              testy, verbose=0)\n"," \n","\treturn test_acc\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","\n","# load models in order\n","\n","members = load_all_models(490, 500)\n","\n","print('Loaded %d models' % len(members))\n","\n","\n","\n","\n","# reverse loaded models so we build the ensemble \n","# with the last models first\n","\n","members = list(reversed(members))\n","\n","\n","\n","\n","# evaluate different numbers of ensembles on hold out set\n","\n","single_scores, ensemble_scores = list(), list()\n","\n","for i in range(1, len(members)+1):\n","\n","\t# evaluate model with i members\n","\n","\tensemble_score = evaluate_n_members(members, i, testX, testy)\n"," \n","\t# evaluate the i'th model standalone\n","\n","\t_, single_score = members[i-1].evaluate(testX, testy, verbose=0)\n"," \n","\t# summarize this step\n","\n","\tprint('> %d: single=%.3f, ensemble=%.3f' % (i, \n","                                             \n","                                             single_score, \n","                                             \n","                                             ensemble_score))\n"," \n","\tensemble_scores.append(ensemble_score)\n"," \n","\tsingle_scores.append(single_score)\n"," \n","\n","\n","\n","# plot score vs number of ensemble members\n","\n","x_axis = [i for i in range(1, len(members)+1)]\n","\n","pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n","\n","pyplot.plot(x_axis, ensemble_scores, marker='o')\n","\n","pyplot.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O-fIx4BZUyy0"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","26.2.6 \n","\n","Linearly and Exponentially Decreasing Weighted Average\n","\n","--\n","\n","linearly decreasing weighted average of models on blobs dataset\n","\n","https://machinelearningmastery.com/polyak-neural-network-model-weight-ensemble/\n","\n","Also look at:\n","\n","https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"pZ7cKDfFUzON"},"source":["# linearly decreasing weighted average \n","# of models on blobs problem\n","\n","from sklearn.datasets import make_blobs\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.models import clone_model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot\n","from numpy import average\n","from numpy import array\n","\n","\n","\n","\n","# load models from file\n","\n","def load_all_models(n_start, n_end):\n","\n","\tall_models = list()\n"," \n","\tfor epoch in range(n_start, n_end):\n","     \n","\t\t# define filename for this ensemble\n","\n","\t\tfilename = 'model_' + str(epoch) + '.h5'\n","\n","\t\t# load model from file\n","\n","\t\tmodel = load_model(filename)\n","  \n","\t\t# add to list of members\n","\n","\t\tall_models.append(model)\n","  \n","\t\tprint('>loaded %s' % filename)\n","  \n","\treturn all_models\n","\n","\n","\n","\n","# create a model from the weights of multiple models\n","\n","def model_weight_ensemble(members, weights):\n","\n","\t# determine how many layers need to be averaged\n","\n","\tn_layers = len(members[0].get_weights())\n"," \n","\t# create an set of average model weights\n","\n","\tavg_model_weights = list()\n"," \n","\tfor layer in range(n_layers):\n","     \n","\t\t# collect this layer from each model\n","\n","\t\tlayer_weights = array([model.get_weights()[layer] for model in members])\n","  \n","\t\t# weighted average of weights for this layer\n","\n","\t\tavg_layer_weights = average(layer_weights, \n","                              \n","                              axis=0, weights=weights)\n","  \n","\t\t# store average layer weights\n","\n","\t\tavg_model_weights.append(avg_layer_weights)\n","  \n","\t# create a new model with the same structure\n","\n","\tmodel = clone_model(members[0])\n"," \n","\t# set the weights in the new\n","\n","\tmodel.set_weights(avg_model_weights)\n"," \n","\tmodel.compile(loss='categorical_crossentropy', \n","               \n","               optimizer='adam', metrics=['accuracy'])\n"," \n","\treturn model\n","\n","\n","\n","\n","# evaluate a specific number of members in an ensemble\n","\n","def evaluate_n_members(members, n_members, testX, testy):\n","\n","\t# select a subset of members\n","\n","\tsubset = members[:n_members]\n","\n","\t# prepare an array of linearly decreasing weights\n","\n","\tweights = [i/n_members for i in range(n_members, 0, -1)]\n","\n","\t# create a new model with the weighted average of all model weights\n","\n","\tmodel = model_weight_ensemble(subset, weights)\n"," \n","\t# make predictions and evaluate accuracy\n","\n","\t_, test_acc = model.evaluate(testX, testy, verbose=0)\n"," \n","\treturn test_acc\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, centers=3, \n","                  \n","                  n_features=2, cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","\n","# load models in order\n","\n","members = load_all_models(490, 500)\n","\n","print('Loaded %d models' % len(members))\n","\n","\n","\n","\n","\n","# reverse loaded models so we build the ensemble with the last models first\n","\n","members = list(reversed(members))\n","\n","\n","\n","# evaluate different numbers of ensembles on hold out set\n","\n","single_scores, ensemble_scores = list(), list()\n","\n","for i in range(1, len(members)+1):\n","\n","\t# evaluate model with i members\n","    \n","\tensemble_score = evaluate_n_members(members, i, testX, testy)\n"," \n","\t# evaluate the i'th model standalone\n","\n","\t_, single_score = members[i-1].evaluate(testX, testy, verbose=0)\n"," \n","\t# summarize this step\n","\n","\tprint('> %d: single=%.3f, ensemble=%.3f' % (i, \n","                                             \n","                                             single_score, \n","                                             \n","                                             ensemble_score))\n"," \n","\tensemble_scores.append(ensemble_score)\n"," \n","\tsingle_scores.append(single_score)\n"," \n","\n","\n","# plot score vs number of ensemble members\n","\n","x_axis = [i for i in range(1, len(members)+1)]\n","\n","pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n","\n","pyplot.plot(x_axis, ensemble_scores, marker='o')\n","\n","pyplot.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"560XliqpZVeP"},"source":["--\n","\n","--\n","\n","--\n","\n","---\n","26.2.6 \n","\n","Linearly and Exponentially Decreasing Weighted Average\n","\n","--\n","B.\n","\n","We can also experiment with an exponential decay of the contribution of models. This requires that a decay rate (alpha) is specified. The example below creates weights for an exponential decay with a decrease rate of 2.\n","\n","--\n","\n","exponentially decaying weighted average of models on blobs problem\n","\n","--\n","https://machinelearningmastery.com/polyak-neural-network-model-weight-ensemble/\n","\n","Also look at:\n","\n","https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/\n","\n","---\n","\n","--\n","\n","--\n","\n","--"]},{"cell_type":"code","metadata":{"id":"0L-75GcRZVy-"},"source":["# exponentially decreasing weighted average of models on blobs problem\n","from sklearn.datasets import make_blobs\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.models import clone_model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from matplotlib import pyplot\n","from numpy import average\n","from numpy import array\n","from math import exp\n","\n","# load models from file\n","\n","def load_all_models(n_start, n_end):\n","\n","\tall_models = list()\n"," \n","\tfor epoch in range(n_start, n_end):\n","     \n","\t\t# define filename for this ensemble\n","\n","\t\tfilename = 'model_' + str(epoch) + '.h5'\n","\n","\t\t# load model from file\n","\n","\t\tmodel = load_model(filename)\n","  \n","\t\t# add to list of members\n","\n","\t\tall_models.append(model)\n","  \n","\t\tprint('>loaded %s' % filename)\n","  \n","\treturn all_models\n","\n","\n","\n","\n","# create a model from the weights of multiple models\n","\n","def model_weight_ensemble(members, weights):\n","\n","\t# determine how many layers need to be averaged\n","\n","\tn_layers = len(members[0].get_weights())\n"," \n","\t# create an set of average model weights\n","\n","\tavg_model_weights = list()\n"," \n","\tfor layer in range(n_layers):\n","     \n","\t\t# collect this layer from each model\n","\n","\t\tlayer_weights = array([model.get_weights()[layer] for model in members])\n","  \n","\t\t# weighted average of weights for this layer\n","\n","\t\tavg_layer_weights = average(layer_weights, \n","                              \n","                              axis=0, weights=weights)\n","  \n","\t\t# store average layer weights\n","\n","\t\tavg_model_weights.append(avg_layer_weights)\n","  \n","\t# create a new model with the same structure\n","\n","\tmodel = clone_model(members[0])\n"," \n","\t# set the weights in the new\n","\n","\tmodel.set_weights(avg_model_weights)\n"," \n","\tmodel.compile(loss='categorical_crossentropy', \n","               \n","               optimizer='adam', \n","               \n","               metrics=['accuracy'])\n"," \n","\treturn model\n","\n","\n","\n","\n","# evaluate a specific number of members in an ensemble\n","\n","def evaluate_n_members(members, n_members, testX, testy):\n","\n","\t# select a subset of members\n","\n","\tsubset = members[:n_members]\n","\n","\t# prepare an array of exponentially decreasing weights\n","\n","\talpha = 2.0\n","\n","\tweights = [exp(-i/alpha) for i in range(1, n_members+1)]\n","\n","\t# create a new model with the weighted average of all model weights\n","\n","\tmodel = model_weight_ensemble(subset, weights)\n"," \n","\t# make predictions and evaluate accuracy\n","\n","\t_, test_acc = model.evaluate(testX, testy, verbose=0)\n"," \n","\treturn test_acc\n","\n","\n","\n","\n","# generate 2d classification dataset\n","\n","X, y = make_blobs(n_samples=1100, \n","                  \n","                  centers=3, n_features=2, \n","                  \n","                  cluster_std=2, \n","                  \n","                  random_state=2)\n","\n","# one hot encode output variable\n","\n","y = to_categorical(y)\n","\n","\n","\n","\n","# split into train and test\n","\n","n_train = 100\n","trainX, testX = X[:n_train, :], X[n_train:, :]\n","trainy, testy = y[:n_train], y[n_train:]\n","\n","\n","\n","# load models in order\n","\n","members = load_all_models(490, 500)\n","\n","print('Loaded %d models' % len(members))\n","\n","\n","\n","# reverse loaded models so we build the \n","# ensemble with the last models first\n","\n","members = list(reversed(members))\n","\n","\n","\n","\n","# evaluate different numbers of ensembles on hold out set\n","\n","single_scores, ensemble_scores = list(), list()\n","\n","for i in range(1, len(members)+1):\n","\n","    # evaluate model with i members\n","\n","    ensemble_score = evaluate_n_members(members, i, testX, testy)\n","\n","    # evaluate the i'th model standalone\n","\n","    _, single_score = members[i-1].evaluate(testX, testy, verbose=0)\n","\n","    # summarize this step\n","\n","    print('> %d: single=%.3f, ensemble=%.3f' % (i, \n","                                                \n","                                                single_score, \n","                                                \n","                                                ensemble_score))\n","\n","    ensemble_scores.append(ensemble_score)\n","\n","    single_scores.append(single_score)\n","\n","# plot score vs number of ensemble members\n","\n","x_axis = [i for i in range(1, len(members)+1)]\n","\n","pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n","\n","pyplot.plot(x_axis, ensemble_scores, marker='o')\n","\n","pyplot.show()\n","\n"],"execution_count":null,"outputs":[]}]}